---
layout: case
title: >-
  Hoger-dimensionale bias in een BERT-gebaseerde disinformatie
  detectie-algoritme
icon: fa-newspaper
summary: >
  The audit commission believes there is a low risk of (higher-dimensional)
  proxy discrimination by the BERT-based disinformation classifier and that the
  particular difference in treatment identified by the quantitative bias scan
  can be justified, if certain conditions apply.
sources: "Applying our self-build unsupervised\_[bias detection tool](https://algorithmaudit.eu/bias_scan)\_on a self-trained BERT-based disinformation classifier on the Twitter1516 dataset. Learn more on\_[Github](https://github.com/NGO-Algorithm-Audit/Bias_scan).\n"
additional_content:
  - title: Stanford's AI Audit Challenge 2023
    content: "This case study, in combination with our\_[bias scan tool](https://algorithmaudit.eu/bias_scan), has been selected as a finalist for\_[Stanford's AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).\n"
    image: /images/supported_by/HAI.png
    image_link: 'https://hai.stanford.edu/ai-audit-challenge-2023-finalists'
    width: 8
  - title: Presentation
    content: "A visual presentation of this case study can be found in this\_[slide deck](https://github.com/NGO-Algorithm-Audit/Bias_scan/blob/master/Main_presentation_joint_fairness_assessment_method.pdf).\n"
    width: 12
algoprudence:
  title: Report
  intro: "Dowload the full report and problem statement\_[here](https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/view?usp=sharing).\n"
  reports:
    - url: >-
        https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/preview
normative_advice_members:
  - name: |
      Anne Meuwese, Professor in Public Law & AI at Leiden University
  - name: >
      Hinda Haned, Professor in Responsible Data Science at University of
      Amsterdam
  - name: |
      Raphaële Xenidis, Associate Professor in EU law at Sciences Po Paris
  - name: |
      Aileen Nielsen, Fellow Law\&Tech at ETH Zürich
  - name: "Carlos Hernández-Echevarría, Assistant Director and Head of Public Policy at the anti-disinformation nonprofit fact-checker\_[Maldita.es](https://maldita.es/maldita-es-journalism-to-not-be-fooled/)\n"
  - name: "Ellen Judson, Head of CASM and Sophia Knight, Researcher, CASM at Britain’s leading cross-party think tank\_[Demos](https://demos.co.uk/)\n"
funded_by:
  - url: 'https://europeanaifund.org/'
    image: /images/supported_by/EUAISFund.png
actions:
  - title: Geselecteerd als finalist voor Stanford's AI Audit Challenge 2023
    description: "Onze\_[bias detectie tool](http://localhost:5173/technical-tools/BDT)\_en bijbehorende algoprudentie van het BERT-gebaseerde disinformatie detectie algoritme is geselecteerd als finalist voor\_[Stanford’s AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).\n\n{{< pdf_frame >}}\n"
    image: /images/supported_by/HAI.png
    date: 06-04-2023
    facets:
      - value: Finalist
        label: Finalist
subtitle: |
  Problemstelling (ALGO:AA:2023:01:P) en adviesdocument (ALGO:AA:2023:01:A)
image: /images/algoprudence/AA202301/Cover.png
form1:
  title: Reageer op deze technische audit
  content: >-
    Uw reactie wordt verstuurd naar het auditing team. Het team leest de reactie
    en, indien het verenigbaar is met Algorithm Audit's richtlijnen voor
    publicatie, wordt de reactie geplaatst in bovenstaande Discussie & debat
    sectie.
  button_text: Verstuur
  backend_link: 'https://formspree.io/f/xyyrjyzr'
  id: case-reaction
  questions:
    - label: Naam
      id: name
      required: true
      type: text
    - label: Affiliated organization
      id: affiliated-organization
      type: text
    - label: Reactie
      id: reaction
      required: true
      type: textarea
    - label: Contactgegevens
      id: contact-details
      required: true
      type: email
      placeholder: Emailadres
---

{{< tab_header width="6" tab1_id="description" tab1_title="Beschrijving van algoprudentie" tab2_id="actions" tab2_title="Acties volgend op algoprudentie" tab3_id="" tab3_title="" default_tab="description" >}}

{{< tab_content_open icon="fa-newspaper" title="Hoger-dimensionale bias in een BERT-gebaseerde disinformatie detectie-algoritme" id="description" >}}

#### Algoprudentienummer

AA:2023:01

#### Samenvatting advies

De adviescommissie is van mening dat er een laag risico is op (hoger-dimensionale) proxydiscriminatie door de BERT-gebaseerde desinformatiedetectie-algoritme. Het concrete waargenomen verschil in ongelijke behandeling van verschillende Twitter-gebruikers kan gelegitimeerd worden, indien bepaalde maatregelen zijn getroffen.

#### Bron van de casus

Toepassing van de door Algorithm Audit zelf-geimplementeerde unsupervised [bias detectie tool](/technical-tools/bdt/) op een zelf-getraind BERT-gebaseerd desinformatiedetectie-algoritme op de veelgebruikte Twitter1516 dataset. Zie ook onze [Github](https://github.com/NGO-Algorithm-Audit/Bias-detection-tool).

#### Stanford's AI Audit Challenge 2023

Deze algoprudentie, in combinatie met de [bias detectie tool,](/technical-tools/bdt/) is onder de naam Joint Fairness Assessment Method (JFAM) geselecteerd als finalist voor [Stanford’s AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).

{{< image id="stanford" width_desktop="6" width_mobile="12" image1="/images/supported_by/HAI.png" alt1="Universiteit Stanford" caption1="Universiteit Stanford" link1="https://hai.stanford.edu/ai-audit-challenge-2023-finalists" >}}

#### Presentatie

Een visuele presentatie van deze case study kan worden gevonden in [dit](https://github.com/NGO-Algorithm-Audit/Bias-detection-tool/blob/master/Main_presentation_joint_fairness_assessment_method.pdf) slide deck.

#### Rapport

De probleemstelling en adviesrapport kan [hier](https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/view?usp=sharing) worden gedownload.

{{< pdf_frame articleUrl1="https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/preview" articleUrl2="" >}}

#### Normatieve adviescommissie

* Anne Meuwese, Hoogleraar staats- en bestuursrecht, Leiden University
* Hinda Haned, Hoogleraar in Responsible Data Science, Universiteit van Amsterdam
* Raphaële Xenidis, Associate Professor in Europees recht, Sciences Po Paris
* Aileen Nielsen, Visiting Assistant Professor, Harvard Law School
* Carlos Hernández-Echevarría, Assistant Director en Head of Public Policy bij anti-disinformatie nonprofit fact-checker [Maldita.es](https://maldita.es/maldita-es-journalism-to-not-be-fooled/)
* Ellen Judson, Head of CASM en Sophia Knight, Onderzoekers bij de partijoverstijgende Britse denktank [Demos](https://demos.co.uk/)

#### Funded by

<br>

{{< image id="funded-by" width_desktop="4" width_mobile="6" image1="/images/supported_by/EUAISFund.png" alt1="European Artificial Intelligence & Society Fund" caption1="European Artificial Intelligence & Society Fund" link1="https://europeanaifund.org/" >}}

{{< tab_content_close >}}

{{< tab_content_open icon="" title="" id="actions" >}}

{{< accordions_area_open id="actions" >}}

{{< accordion_item_open image="/images/supported_by/sidn.png" title="Ondersteuning voor verdere ontwikkeling" id="sidn" date="01-12-2023" tag1="financiering" tag2="open source" tag3="AI auditing tool" >}}

##### Beschrijving

Het [SIDN Fonds](https://www.sidnfonds.nl/projecten/open-source-ai-auditing) ondersteunt Algorithm Audit om de bias detectie tool verder te ontwikkelen. Op 01-01-2024 is een [team](/nl/about/teams/#bdt) gestart dat de tool verder gaat ontwikkelen.

{{< accordion_item_close >}}

{{< accordion_item_open title="Finalist Stanford's AI Audit Challenge 2023" image="/images/supported_by/HAI.png" id="ai_audit_challenge" date="28-04-2023" tag1="finalist" >}}

##### Beschrijving

Deze algoprudentie, in combinatie met de [bias detectie tool,](/technical-tools/bdt/) is onder de naam Joint Fairness Assessment Method (JFAM) geselecteerd als finalist voor [Stanford’s AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).

{{< accordion_item_close >}}

{{< accordions_area_close >}}

{{< tab_content_close >}}

{{< form1 >}}
