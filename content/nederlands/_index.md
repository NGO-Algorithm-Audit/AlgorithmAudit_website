---
Banner:
  title: '_Algoprudentie_:'
  image: images/main_illustration.svg
  content: Algorithm Audit is een Europees kennisplatform voor het testen van AI-bias en normatieve AI-standaarden. We creëren algoprudentie door het samenbrengen van deliberatieve auditcommissies. Deze commissies geven onafhankelijk normatief advies over ethische vraagstukken bij inzet van algoritmes
  button:
    enable: True
    label: Lees onze nieuwe white paper
    link: knowledge_base/white_paper_dsa_delegated_regulation_feedback/
What_we_do:
  enable: true
  title: Onderscheidend in
  feature_item:
  - name: Normatief advies
    icon: fas fa-search
    content: Auditcommissies geven normatief advies over concrete ethische vraagtukken die zich voordoen bij gebruik van algoritmes
  - name: Onafhankelijk
    icon: fas fa-star-of-life
    content: We werken zonder winstoogmerk. Onze werkafspraken garanderen onafhankelijkheid, kwaliteit en diversiteit van onze auditcommissies en bijbehorend normatief advies
  - name: Ethiek voorbij compliance
    icon: fas fa-leaf
    content: We helpen organisaties die zich committeren aan de verantwoorde inzet van algoritmes met biastoetsing en het invullen van open juridische normen
  - name: Public kennisopbouw
    icon: fab fa-slideshare
    content: Al onze casuïstiek en bijbehorend advies (_algoprudentie_) is [<span style="color:#005aa7">openbaar</span>](/algoprudence). Zo dragen we bij aan publieke kennisopbouw over de verantwoorde inzet van algoritmes
  - name: Techno-ethische jurisprudentie
    icon: fas fa-book-reader
    content: Belanghebbenden kunnen van onze algoprudentie leren, het helpen verbeteren en het inzetten als referentiemateriaal bij soortgelijke vraagstukken
  - name: 'Krachten bundelen'
    icon: fas fa-hands-helping
    content: Publieke en private organisaties lopen tegen vergelijkbare uitdagingen aan. Wij jagen het collectieve leerproces over de verantwoorde inzet van algoritmes aan door overheid, bedrijfsleven, NGOs en wetenschap met elkaar te verbinden
Supported_by:
  enable: true
  Suported_by:
  - title: Partners
    img_SIDN:
    - images/sidn.png
    img_EUAIS:
    - images/EUAISFund.png
    img_NLAIC:
    - images/NLAIC.png
    img_BZK:
    - images/BZK.jpg
    img_HAI:
    - images/HAI.png
How_we_work:
  enable: true
  service_item:
  - title: Werkwijze
    images:
    - images/howwework.svg
    content: ''
    button:
      enable: true
      label: Onderzochte algoritmes
      link: nl/algoprudence
With_whom_we_work:
  enable: true
  service_item:
  - title: Met wie werken we samen
    images:
    - images/howwework.svg
    content: We werken samen met internationale experts met verschillende achtergronden, o.a. ethici, juristen en datawetenschappers. De samenstelling van auditcommissies verschilt per vraagstuk. Commissieleden zijn verbonden aan een academische instellingen, zijn domeinexpert of worden zelf onderworpen aan het algoritme. Het team van Algorithm Audit brengt achtergrondinformatie samen over de casus, waarna de experts eerst individueel en daarna gezamenlijk een verdiepend onderzoek doen. Ons team stelt een adviesrapport op dat de (verschillende) standpunten van de commissie samenvat. Het uiteindelijke gepubliceerde advies wordt ondersteund door alle commissieleden.
    button:
      enable: false
      label: Cases we work on
      link: "nl/algoprudence"
Why_we_exist:
  enable: true
  content: Current legislation falls short to
  new_items:
  - title: Waarom we bestaan
    subtitle: EU AI beleidsontwikkelingen

    # menu
    tab1: AI verordening
    tab2: DSA
    tab3: Avg
    tab4: IAMA
    tab5: Toezichthouderslandschap 
    tab6: Gesloten ontwikkel-omgeving
    tab7: Toezicht-houders landschap 

    # intro
    intro_content: Het is hard nodig ervaringen te delen hoe algoritmes verantwoord ontwikkeld kunnen worden. Bestaande en komende wetgeving lost niet alle vragen op. Waarom niet?
    
    #AI Act
    ai_act_content:  The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1623335154975&uri=CELEX%3A52021PC0206" target="_blank">AI Act </a> imposes broad new responsibilities to control risks from AI systems without at the same time laying down specific standards they are expected to meet. For instance<a>:</a>
    ai_act_tag_conformity: Conformity assessment (Art. 43)<a>:</a>
    ai_act_content_conformity: The proposed route for internal control relies too much on the self-reflective capacities of producers to assess AI quality management, risk management and bias. Resulting in subjective best-practices;
    ai_act_tag_risk: Risk- and quality management systems (Art. 9 and 17)<a>:</a>
    ai_act_content_risk: Requirements set out for risk management systems and quality management systems remain too generic. For example, it does not provide precise guidelines how to identify and mitigate ethical issues such as algorithmic discrimination;
    ai_act_tag_standards: Normative standards<a>:</a>
    ai_act_content_standards: To realize AI harmonization across the EU, publicly available technical and normative best-practices for fair AI are urgently needed.
    ai_act_white_paper: Read our algoprudence on proxy discrimination
    ai_act_image: images/AIA.png

    #DSA
    dsa_content: The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52020PC0825" target="_blank">Digital Services Act (DSA) </a> lacks provisions to disclose normative methodological choices that underlie general purpose AI systems. For instance<a>:</a>
    dsa_tag_risk: Risk definitions<a>:</a>
    dsa_content_risk: Article 9 of the <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits_en" target="_blank">Delegated Regulation (DR)</a> for independent third party auditing (as mandated under DSA Art. 37) specifies that “audit risk analysis shall consider _inherent risk_, _control risk_ and _detection risk_”. More specific guidance should be provided in Art. 2 of the DR how risks relating to subjective concepts, such as “...the nature, the activity and the use of the audited service”, can be assessed;  
    dsa_tag_template: Audit template<a>:</a>
    dsa_content_template: Pursuant to Article 5(1)(a) of the DR, Very Large Open Platforms (VLOPs) and Very Large Online Search Engines (VLOSEs) shall transmit to third-party auditing organisations “benchmarks used [...] to assert or monitor compliance [...], as well as supporting documentation”. We argue that the normative considerations underlying the selection of these benchmarks should be asked out more decisively in this phase of the audit. Therefore, we asked the European Commission (EC) to add this dimension to Question 3(a) of Section D.1 _Audit conclusion for obligation Subsection_ II. _Audit procedures and their results_;
    dsa_white_paper: Read our feedback to the Europen Commission on DSA Art. 37 Delegated Regulation  
    dsa_content_feedback: <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits/feedback_en?p_id=32081201" target="_blank">Feedback</a> submitted to the European Commission (EC) on DSA Art. 37 DR showcasts that<a>:</a>
    dsa_content_feedback_tab1: Private auditors (like PwC and Deloitte) warn that the lack of guidance on criteria against which to audit poses a risk of subjective audits;
    dsa_content_feedback_tab2: Tech companies (like Snap and Wikipedia) raise concerns about the industry's lack of expertise to audit specific AI products, like company-tailored timeline recommender systems.
    dsa_image: images/DSA.jpg

    #GDPR
    gdpr_content1: Conditions provided in <a href="https://gdpr-info.eu/art-22-gdpr/" target="_blank">GDPR Article 22(2)</a>
      under which automated decision-making (ADM) and profiling is allowed are open
      for broad interpretation. Allowing wide-ranging ADM under the sole condition
      of contract agreement opens the door for large scale unethical algorithmic practices
      without accountability and public awareness.
    gdpr_content2: <a href="https://gdpr-info.eu/art-35-gdpr/" target="_blank">GDPR Article 35(9)</a>
      stimulates controller to seek the views of data subjects or their representatives on the intended processing, without prejudice to the protection of commercial or public interests or the security of processing operations.
    gdpr_image: images/gdpr.svg

    #Regulators
    regulator_content: <a href="https://www.rekenkamer.nl/onderwerpen/algoritmes-digitaal-toetsingskader/ethiek" target="_blank">Perspective 3.1.1</a> in the Guidelines for Algorithms of the Dutch Court of Auditors argues that
      ethical algorithms are not allowed to “discriminate and that bias should be
      minimised”. Missing from this judgment is a discussion of what precisely constitutes
      bias in the context of algorithms and what would be appropriate methods to ascertain
      and mitigate algorithmic discrimination. In the absence of a clear ethical framework, it
      is up to organizations to formulate context-sensitive approaches to combat discrimination.
    sdg_image: images/SDG16.png

    #HRIA
    hria_content: The <a href="https://www.rijksoverheid.nl/documenten/rapporten/2021/02/25/impact-assessment-mensenrechten-en-algoritmes" target="_blank">Impact Assessment Human Rights and Algorithms (IAMA)</a> and the <a href="https://www.rijksoverheid.nl/documenten/rapporten/2021/06/10/handreiking-non-discriminatie-by-design" target="_blank">Handbook for
      Non-Discrimination</a>, both developed by the Dutch government, assess discriminatory
      practice mainly by asking questions that are meant to stimulate self-reflection.
      It does not provide answers or concrete guidelines how to realise ethical algorithms.
    hria_image: images/FRIA.png
    content1:  We believe a case-based and context sensitive approach is indispensable to develop ethical algorithms. One should not expect top-down regulation and
      legislation to solve all ethical problems in AI and machine learning. Taking
      all contested algorithmic cases to court is practically infeasible. More importantly,
      organizations will always carry their own responsibility for ethical algorithms
      within and beyond the obligation of legal compliance. Hence, new bottom-up initiatives
      like Algorithm Audit are necessary to support these organizations and to strengthen
      ethical practice in ADM and decision support.
      

      We provide a platform where experts
      in AI ethics from various disciplines can interact with, learn from and steer actual algorithmic practice
      and surrounding ethical concerns. We increase public knowledge and stimulate
      an informed and open debate about what ethical algorithms we desire as a society in various
      contexts. Our audit commissions shape future public values through discussion and deliberation. As such, Algorithm Audit contributes in the digital realm to SDG16 – Peace, Justice and
      Strong Institutions. 
Get_in_touch:
  enable: true
  title: Denk mee
  content: Wil je advies over een algoritme? Of wil je nieuwe ideeën met ons uitwerken? Neem contact op.

---
