---
Banner:
  # title: '_Algoprudentie_:'
  image1: images/EU_knowledge_platform.svg
  image2: images/main_illustration.svg
  # content: Algorithm Audit is een Europees kennisplatform voor het testen van AI-bias en normatieve AI-standaarden. We creëren algoprudentie door het samenbrengen van deliberatieve auditcommissies. Deze commissies geven onafhankelijk normatief advies over ethische vraagstukken bij inzet van algoritmes
  button:
    enable: True
    label: Lees onze nieuwe white paper
    link: knowledge_base/white_paper_dsa_delegated_regulation_feedback/
What_we_do:
  enable: true
  title: Onderscheidend in
  feature_item:
  - name: Normatief advies
    icon: fas fa-search
    content: Auditcommissies geven normatief advies over concrete ethische vraagtukken die zich voordoen bij gebruik van algoritmes
  - name: Onafhankelijk
    icon: fas fa-star-of-life
    content: We werken zonder winstoogmerk. Onze werkafspraken garanderen onafhankelijkheid, kwaliteit en diversiteit van onze auditcommissies en bijbehorend normatief advies
  - name: Ethiek voorbij compliance
    icon: fas fa-leaf
    content: We helpen organisaties die zich committeren aan de verantwoorde inzet van algoritmes met biastoetsing en het invullen van open juridische normen
  - name: Public kennisopbouw
    icon: fab fa-slideshare
    content: Al onze casuïstiek en bijbehorend advies (_algoprudentie_) is [<span style="color:#005aa7">openbaar</span>](/algoprudence). Zo dragen we bij aan publieke kennisopbouw over de verantwoorde inzet van algoritmes
  - name: Techno-ethische jurisprudentie
    icon: fas fa-book-reader
    content: Belanghebbenden kunnen van onze algoprudentie leren, het helpen verbeteren en het inzetten als referentiemateriaal bij soortgelijke vraagstukken
  - name: 'Krachten bundelen'
    icon: fas fa-hands-helping
    content: Publieke en private organisaties lopen tegen vergelijkbare uitdagingen aan. Wij jagen het collectieve leerproces over de verantwoorde inzet van algoritmes aan door overheid, bedrijfsleven, NGOs en wetenschap met elkaar te verbinden
Supported_by:
  enable: true
  Suported_by:
  - title: Partners
    img_SIDN:
    - images/sidn.png
    img_EUAIS:
    - images/EUAISFund.png
    img_NLAIC:
    - images/NLAIC.png
    img_BZK:
    - images/BZK.jpg
    img_HAI:
    - images/HAI.png
How_we_work:
  enable: true
  service_item:
  - title: Werkwijze
    images:
    - images/howwework.svg
    content: ''
    button:
      enable: true
      label: Onderzochte algoritmes
      link: nl/algoprudence
AA_expert_hub:
  enable: true
  service_item:
  - title: Met wie werken we samen
    expert_hub_title: AI Auditing Expert hub
    expert_hub_content: Deel ervaringen met de internationale AI auditing community. Word onderdeel van het Slack channel<span>.</span>
    images:
    - images/slack.png
    content: We werken samen met internationale experts van verschillende achtergronden, o.a. ethici, juristen en datawetenschappers. De samenstelling van auditcommissies verschilt per vraagstuk. Commissieleden zijn verbonden aan een academische instellingen, zijn domeinexpert of worden onderworpen aan het algoritme. 
    button:
      enable: true
      label: Aanmelden
      link: "mailto:experthub@algorithmaudit.eu?subject=Request to join AI Auditing Expert Hub Slack channel&body=Hi Algorithm Audit,\n\nCan you add me to the Slack channel?\n\nName:\nAffiliation:\nMotivation to join:\n\nBest,"
Why_we_exist:
  enable: true
  content: Current legislation falls short to
  new_items:
  - title: Waarom we bestaan
    subtitle: Nationale en Europese AI beleidsontwikkelingen

    # menu
    tab1: AI Act
    tab2: DSA
    tab3: Avg
    tab4: Bestuursrecht
    tab5: IAMA
    tab6: Algoritmeregisters
    tab7: Toezichthouderslandschap 
    tab8: Bestuurs-recht 
    tab9: Algoritme-register 

    # intro
    intro_content: Het is hard nodig ervaringen te delen hoe algoritmes verantwoord ontwikkeld kunnen worden. Bestaande en komende wetgeving lost niet alle vragen op. Waarom niet?
    
   #AI Act
    ai_act_title: AI Verordening
    ai_act_content:  The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1623335154975&uri=CELEX%3A52021PC0206" target="_blank">AI Act </a> imposes broad new responsibilities to control risks from AI systems without at the same time laying down specific standards they are expected to meet. For instance<span>:</span>
    ai_act_tag_conformity: Conformity assessment (Art. 43)<span>:</span>
    ai_act_content_conformity: The proposed route for internal control relies too much on the self-reflective capacities of producers to assess AI quality management, risk management and bias. Resulting in subjective best-practices;
    ai_act_tag_risk: Risk- and quality management systems (Art. 9 and 17)<span>:</span>
    ai_act_content_risk: Requirements set out for risk management systems and quality management systems remain too generic. For example, it does not provide precise guidelines how to identify and mitigate ethical issues such as algorithmic discrimination;
    ai_act_tag_standards: Normative standards<span>:</span>
    ai_act_content_standards: To realize AI harmonization across the EU, publicly available technical and normative best-practices for fair AI are urgently needed.
    ai_act_white_paper: Read our algoprudence on proxy discrimination
    ai_act_image: images/AIA.png

    #DSA
    dsa_title: Verordening digitale diensten
    dsa_content: The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52020PC0825" target="_blank">Digital Services Act (DSA) </a> lacks provisions to disclose normative methodological choices that underlie general purpose AI systems. For instance<span>:</span>
    dsa_tag_risk: Risk definitions<span>:</span>
    dsa_content_risk: Article 9 of the <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits_en" target="_blank">Delegated Regulation (DR)</a> for independent third party auditing (as mandated under DSA Art. 37) specifies that “audit risk analysis shall consider _inherent risk_, _control risk_ and _detection risk_”. More specific guidance should be provided in Art. 2 of the DR how risks relating to subjective concepts, such as “...the nature, the activity and the use of the audited service”, can be assessed;  
    dsa_tag_template: Audit template<span>:</span>
    dsa_content_template: Pursuant to Article 5(1)(a) of the DR, Very Large Open Platforms (VLOPs) and Very Large Online Search Engines (VLOSEs) shall transmit to third-party auditing organisations “benchmarks used [...] to assert or monitor compliance [...], as well as supporting documentation”. We argue that the normative considerations underlying the selection of these benchmarks should be asked out more decisively in this phase of the audit. Therefore, we asked the European Commission (EC) to add this dimension to Question 3(a) of Section D.1 _Audit conclusion for obligation Subsection_ II. _Audit procedures and their results_;
    dsa_white_paper: Read our feedback to the Europen Commission on DSA Art. 37 Delegated Regulation  
    dsa_content_feedback: <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits/feedback_en?p_id=32081201" target="_blank">Feedback</a> submitted to the European Commission (EC) on DSA Art. 37 DR showcasts that<span>:</span>
    dsa_content_feedback_tab1: Private auditors (like PwC and Deloitte) warn that the lack of guidance on criteria against which to audit poses a risk of subjective audits;
    dsa_content_feedback_tab2: Tech companies (like Snap and Wikipedia) raise concerns about the industry's lack of expertise to audit specific AI products, like company-tailored timeline recommender systems.
    dsa_image: images/DSA.jpg

    #GDPR
    gdpr_title: Algemene verordening gegevensbescherming (Avg)
    gdpr_content1: Organizations that develop algorithms do often not comply with GDPR provisions that foster participatory algorithm development. For example<span>:</span>
    gdpr_content2: Besides, the current regulation only partially specifies measures to safeguard algorithmic decision-making. For instance<span>:</span> 
    gdpr_tag_dpia: Participatroy DPIA (art. 35 sub 9)<span>:</span>
    gdpr_content_dpia: This provision mandates that in cases where a Data Privacy Impact Assessment (DPIA) is obligatory, the opinions of data subjects regarding the planned data processing shall be seeked. This is a powerful legal mechanism to foster collaborative algorithm development. Nevertheless, the inclusion of data subjects in this manner is scarcely observed in practice;
    gdpr_tag_profiling: Profiling (recital 71)
    gdpr_content_profiling: is broadly defined as<span>:</span> “to analyse or predict aspects concerning the data subject’s performance at work, economic situation, health, personal preferences or interests, reliability or behaviour, location or movements”. However, the approval of profiling, particularly when “authorised by Union or Member State law to which the controller is subject, including fraud monitoring”, grants public and private entities significant flexibility to integrate algorithmic decision-making derived from diverse types of profiling. This wide latitude raises concerns about the potential for excessive consolidation of personal data and the consequences of algorithmic determinations;
    gdpr_tag_ADM: Automated decision-making (art. 22 sub 2)<span>:</span>
    gdpr_content_ADM: Allowing wide-ranging automated decision-making (ADM) and profiling under the sole condition of contract agreement opens the door for large scale unethical algorithmic practices without accountability and public awareness.
    gdpr_image: images/gdpr.svg

    #Administrative law
    administrative_law_title: Algemene wet bestuursrecht
    administrative_law_content: Unifying principles of sound administration with algorithmic methods is challenging. For instance<span>:</span>
    administrative_law_tag_motivation: Motivation principle<span>:</span>
    administrative_law_content_motivation: Governmental institutions must always provide clear explanations for their decisions. However, when machine learning is employed, such as in variable selection for risk profiling, this transparency may be obscured. This leads to the question of how far arguments based on probability distributions are acceptable as explanations for why certain citizens are chosen for a particular profile.
    algoprudence_rotterdam: Read Algoprudence AA:2023:02 for a review of xgboost model used for risk profiling variable selection
    administrative_law_image: images/Awb.jpg

    #Regulatory landscape
    regulator_title: Toezichthouderslandschap
    regulator_content: <a href="https://www.rekenkamer.nl/onderwerpen/algoritmes-digitaal-toetsingskader/ethiek" target="_blank">Perspective 3.1.1</a> in the Guidelines for Algorithms of the Dutch Court of Auditors argues that
      ethical algorithms are not allowed to “discriminate and that bias should be
      minimised”. Missing from this judgment is a discussion of what precisely constitutes
      bias in the context of algorithms and what would be appropriate methods to ascertain
      and mitigate algorithmic discrimination. In the absence of a clear ethical framework, it
      is up to organizations to formulate context-sensitive approaches to combat discrimination.
    regulator_image: images/FRIA.png

    #Human rights and fundamental rights impact assessments
    hria_title: Impact Assessments Mensenrechten en Algoritmes
    hria_content: The <a href="https://www.rijksoverheid.nl/documenten/rapporten/2021/02/25/impact-assessment-mensenrechten-en-algoritmes" target="_blank">Impact Assessment Human Rights and Algorithms (IAMA)</a> and the <a href="https://www.rijksoverheid.nl/documenten/rapporten/2021/06/10/handreiking-non-discriminatie-by-design" target="_blank">Handbook for
      Non-Discrimination</a>, both developed by the Dutch government, assess discriminatory
      practice mainly by asking questions that are meant to stimulate self-reflection.
      It does not provide answers or concrete guidelines how to realise ethical algorithms.
    hria_image: images/FRIA.png

    #AI registers
    AI_register_title: Algoritmeregisters
    AI_register_content: ..
    AI_register_image: images/knowledge_base.svg

Get_in_touch:
  enable: true
  title: Denk mee
  content: Wil je advies over een algoritme? Of wil je nieuwe ideeën met ons uitwerken? Neem contact op.

---
