---
title: AI beleidsobservatorium
subtitle: >
  Er zijn verschillende beleidsinitiatieven om algoritmes en AI verantwoord in te zetten. 
  Op deze pagina wordt informatie bijgehouden over belangrijke Europese en nationale initiatieven, inclusief 
  materiaal dat Algorithm Audit over het onderwerp heeft ontwikkeld.
image: /images/svg-illustrations/knowledge_base.svg
---

{{< tab_header width="2" default_tab="AIAct" tab1_id="AIAct" tab1_title="AI-verordening" tab2_id="GDPR" tab2_title="AVG" tab3_id="administrative-law" tab3_title="Bestuursrecht" tab4_id="FRIA" tab4_title="IAMA" tab5_id="DSA" tab5_title="DSA">}}

{{< tab_content_open id="AIAct" icon="fa-newspaper" title="AI-verordening" >}}

De <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1623335154975&uri=CELEX%3A52021PC0206" target="_blank">AI-verordening</a> legt brede nieuwe verantwoordelijkheden op om risico's van AI-systemen te beheersen, maar specifieke normen voor de verantwoorde inzet van algoritmes ontbreken vooralsnog. Bijvoorbeeld:

* **Risico- en kwaliteitmanagementsysteem (Art. 9 and 17) –** Vereisten voor risico- en kwaliteitmanagement systemen blijven te generiek. De vereisten stellen bijvoorbeeld dat AI systemen niet mogen discrimineren en dat ethische risico's in kaart moeten worden gebracht. Er wordt echter niet toegelicht hoe discriminatie kan worden vastgesteld of hoe waardenspanningen beslecht kunnen worden;
* **Conformiteitsassessment (Art. 43) –** De AI Verordening leunt zwaar of interne controles en mechanismen die zelf-reflectie moeten bevorderen om AI systemen op een verantwoorde wijze in te zetten. Dit leidt echter tot subjectieve keuzen. Meer institutionele duiding is nodig over normatieve vraagstukken;
* **Normatieve standaarden –** Enkel technische standaarden voor AI-systemen, zoals de Europese Commissie  standaardiseringsorganisaties CEN-CENELEC heeft verzocht te ontwikkelen, zijn onvoldoende om voor de verantwoorde inzet van AI systemen. Publieke kennis over technische én normatieve oordeelsvorming over verantwoorde AI-systemen is hard nodig. Maar juist hier is een gebrek aan.

Als lid van het Nederlands Normalisatie Instituut NEN draagt Stichting Algorithm Audit bij aan het Europese debat hoe fundamentele rechten gecoreguleerd kunnen worden door productveiligheidsregulatie zoals de AI-verordening.

{{< button button_text="Gebruik onze AI-verordening implementatietool" button_link="/nl/technical-tools/implementation-tool/" >}}

{{< tab_content_close >}}

{{< tab_content_open icon="fa-newspaper" title="Digital Services Act (DSA)" id="DSA" >}}

The [Digital Services Act (DSA)](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52020PC0825) lacks provisions to disclose normative methodological choices that underlie the AI systems the DSA tries to regulate. For instance:

* **Risk definitions –** Article 9 of the <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits_en" target="_blank">Delegated Regulation (DR)</a> for independent third party auditing (as mandated under DSA Art. 37) specifies that “audit risk analysis shall consider inherent risk, control risk and detection risk”. More specific guidance should be provided in Art. 2 of the DR how risks relating to subjective concepts, such as “…the nature, the activity and the use of the audited service”, can be assessed;
* **Audit templates –** Pursuant to Article 5(1)(a) of the DR, Very Large Open Platforms (VLOPs) and Very Large Online Search Engines (VLOSEs) shall transmit to third-party auditing organisations “benchmarks used \[…] to assert or monitor compliance \[…], as well as supporting documentation”. We argue that the normative considerations underlying the selection of these benchmarks should be asked out more decisively in this phase of the audit. Therefore, we asked the European Commission (EC) to add this dimension to Question 3(a) of Section D.1 Audit conclusion for obligation Subsection II. Audit procedures and their results;
* **Insufficient knowledge how to audit AI –** <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits/feedback_en?p_id=32081201" target="_blank">Feedback</a> submitted to the European Commission (EC) on DSA Art. 37 DR reveals that:
  * Private auditors (like PwC and Deloitte) warn that the lack of guidance on criteria against which to audit poses a risk of subjective audits;
  * Tech companies (like Snap and Wikipedia) raise concerns about the industry’s lack of expertise to audit specific AI products, like company-tailored timeline recommender systems.

#### Read Algorithm Audit's feedback to the Europen Commission on DSA Art. 37 Delegated Regulation

{{< embed_pdf url="/pdf-files/policy-observatory/20230705_DSA_delegated_regulation.pdf" width_desktop_pdf="6" width_mobile_pdf="12" >}}

{{< button button_text="Read the white paper" button_link="/knowledge-platform/knowledge-base/white_paper_dsa_delegated_regulation_feedback/" >}}

{{< tab_content_close >}}

{{< tab_content_open icon="fa-newspaper" title="Algemene Verordening Gegevensverwerking (AVG)" id="GDPR" >}}

De AVG kent sterke punten met betrekking tot het horen van belanghebbenden, al kent het ook hiaten met betrekking tot uitvoerbaarheid en rechtsonzekerheid over begrippen als 'volledige geautomatiseerde besluitvorming'.

* <a href="https://gdpr-info.eu/art-35-gdpr/" target="\_blank"> Participatoire Data Privacy Impact Assessment (DPIA) (art. 35 sub 9)</a> – Dit artikel vereist dat wanneer een Data Privacy Impact Assessment (DPIA) verplicht is, data subjecten gehoord zullen worden over het dataverwerkingsproces. Dit is een krachtig mechanisme om de visie van diverse belanghebbenden bijeen te brengen. In de praktijk wordt echter nauwelijks aan de verplichting voldaan;
* <a href="https://gdpr-info.eu/art-22-gdpr/" target="\_blank"> Geautomatiseerde besluitvorming (art. 22 sub 2)</a> – Aanhoudende rechtsonzekerheid wat exact de reikwijdte is van 'geautomatiseerde besluitvorming' en 'betekenisvolle menselijke tussenkomst' gegeven het <a href="[https://](https://curia.europa.eu/juris/liste.jsf?num=C-634/21)" target="_blank">Schüfa arrest</a> van het Hof van Justitie van de European Union (CJEU).

#### Artikel dat een overzicht biedt van interactie tussen AVG en de AI-verordening met betrekking tot dataverzameling voor debiasing

{{< embed_pdf url="/pdf-files/policy-observatory/2023_VanBekkum_Using sensitive data to prevent discrimination by AI.pdf" width_desktop_pdf="6" width_mobile_pdf="12" >}}

{{< tab_content_close >}}

{{< tab_content_open icon="fa-newspaper" title="Bestuursrecht" id="administrative-law" >}}

De Algemene wet bestuursrecht (Awb) biedt een wettelijk kader voor algoritme-gedreven besluitvorming door overheidsorganisaties. Principes hiervoor zijn gecodificeerd in de Algemene beginselen voor behoorlijk bestuur (Abbb). Algorithm Audit vindt dat deze principes nader gecontextualiseerd moeten worden om daadwerkelijk kaderstellend te zijn voor de algoritmische praktijk. In het bijzonder met betrekking tot:

* **Motiversingsbeginsel:** Op basis van het motiveringsbeginsel moet voldoende duidelijk zijn op basis waarvan en waarom een bestuursorgaan een besluit neemt. Als zodanig heeft het motiveringsbeginsel een meta-karakter: het is dit beginsel dat de belanghebbende de gelegenheid geeft te weten te komen of in het kader van een bepaald besluit de andere beginselen ook zijn nageleefd. 
* **Zorgvuldigheidsbeginsel:** Verplicht overheidsorganisaties ertoe de omstandigheden te scheppen waarin een juist besluit kan worden genomen. Zo moeten relevante feiten en af te wegen belangen bekend zijn. Er moet een geschikte methode worden gebruikt om de belangen af te wegen en er dient een volledige belangenafweging plaats te vinden;
* **Beginsel van fair play:** Het beginsel van fair play, ofwel correcte bejegening, dat als een verbod van vooringenomenheid van artikel 2:4 Awb deels gecodificeerd is, gaat over het zonder partijdigheid vervullen van taken door een bestuursorgaan. Het biedt het voordeel dat het ook buiten de specifieke besluitcontext van toepassing is. 

#### Lees het artikel Hoe ‘algoprudentie’ kan bijdragen aan de verantwoorde inzet van ML-algoritmes

{{< embed_pdf url="/pdf-files/policy-observatory/20240313_NJB10_Hoe algoprudentie kan bijdragen aan een verantwoorde inzet van machine learning-algoritmes.pdf" width_desktop_pdf="6" width_mobile_pdf="12" >}}

{{< tab_content_close >}}

{{< tab_content_open icon="fa-newspaper" title="FRIA" id="FRIA" >}}

Met de jaren zijn er in binnen- en buitenland verschillende Fundamental Rights Impact Assessments (FRIAs) ontwikkeld. FRIAs stimuleren zelfreflectie met betrekking tot de verantwoorde inzet van algoritmes, al geven de assessments geen antwoorden of concrete richtlijnen wat wel en niet een verantwoorde algoritmische toepassing betreft.

#### Lees Algorithm Audit's vergelijkende onderzoek van 10 FRIAs

{{< embed_pdf url="/pdf-files/policy-observatory/20240918_Comparative review 10 FRIAs Algorithm Audit.pdf" width_desktop_pdf="6" width_mobile_pdf="12" >}}

{{< tab_content_close >}}
