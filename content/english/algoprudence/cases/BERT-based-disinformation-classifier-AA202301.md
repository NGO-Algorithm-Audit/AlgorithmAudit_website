---
title: >-
  Higher-dimensional proxy bias in a BERT-based disinformation classifier
  (AA:2023:01)
subtitle: ''
image: /images/algoprudence/AA202301/Cover.png
form:
  title: React to the normative judgement of the advice commission
  button_text: Submit
  backend_link: 'https://formspree.io/f/xyyrjyzr'
  id: case-reaction
  questions:
    - label: Name
      id: name
      type: text
    - label: Affiliated organization
      id: affiliated-organization
      type: text
    - label: Reaction
      id: reaction
      type: textarea
    - label: Contact detail
      id: contact-details
      type: email
      placeholder: Mail address
layout: case
icon: fa-newspaper
summary: >
  The advice commission believes there is a low risk of (higher-dimensional)
  proxy discrimination by the BERT-based disinformation classifier and that the
  particular difference in treatment identified by the quantitative bias scan
  can be justified, if certain conditions apply.
sources: "Applying our self-build unsupervised\_[bias detection tool](https://algorithmaudit.eu/bias_scan)\_on a self-trained BERT-based disinformation classifier on the Twitter1516 dataset. Learn more on\_[Github](https://github.com/NGO-Algorithm-Audit/Bias_scan).\n"
additional_content:
  - title: Stanford's AI Audit Challenge 2023
    content: "This case study, in combination with our\_[bias scan tool](https://algorithmaudit.eu/bias_scan), has been selected as a finalist for\_[Stanford's AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).\n"
    image: /images/supported_by/HAI.png
    image_link: 'https://hai.stanford.edu/ai-audit-challenge-2023-finalists'
    width: 8
  - title: Presentation
    content: "A visual presentation of this case study can be found in this\_[slide deck](https://github.com/NGO-Algorithm-Audit/Bias_scan/blob/master/Main_presentation_joint_fairness_assessment_method.pdf).\n"
    width: 12
algoprudence:
  title: Report
  intro: "Dowload the full report and problem statement\_[here](https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/view?usp=sharing).\n"
  reports:
    - url: >-
        https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/preview
normative_advice_members:
  - name: |
      Anne Meuwese, Professor in Public Law & AI at Leiden University
  - name: >
      Hinda Haned, Professor in Responsible Data Science at University of
      Amsterdam
  - name: |
      Raphaële Xenidis, Associate Professor in EU law at Sciences Po Paris
  - name: |
      Aileen Nielsen, Fellow Law\&Tech at ETH Zürich
  - name: "Carlos Hernández-Echevarría, Assistant Director and Head of Public Policy at the anti-disinformation nonprofit fact-checker\_[Maldita.es](https://maldita.es/maldita-es-journalism-to-not-be-fooled/)\n"
  - name: "Ellen Judson, Head of CASM and Sophia Knight, Researcher, CASM at Britain’s leading cross-party think tank\_[Demos](https://demos.co.uk/)\n"
funded_by:
  - url: 'https://europeanaifund.org/'
    image: /images/supported_by/EUAISFund.png
actions:
  - id: ai_audit_2023
    title: Finalist selection Stanford's AI Audit Challenge 2023
    description: >
      Our [bias detection tool](/technical-tools/BDT) and this case study have
      been selected as a finalist for [Stanford's AI Audit Challenge
      2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).


      {{< pdf_frame articleUrl="/pdf-files/BDT_main_presentation_JFAM.pdf" >}}
    image: /images/supported_by/HAI.png
    date: 14-02-2024
    facets:
      - value: finalist
        label: finalist
---

{{< tab_header width="6" tab1_id="description" tab1_title="Description" tab2_id="actions" tab2_title="Actions" tab3_id="" tab3_title="" default_tab="description" >}}

{{< tab_content_open icon="fa-newspaper" title="Description" id="description" >}}

#### Summary advice

The advice commission believes there is a low risk of (higher-dimensional) proxy discrimination by the BERT-based disinformation classifier and that the particular difference in treatment identified by the quantitative bias scan can be justified, if certain conditions apply.

#### Source of case

Applying our self-build unsupervised [bias detection tool](https://algorithmaudit.eu/bias_scan) on a self-trained BERT-based disinformation classifier on the Twitter1516 dataset. Learn more on [Github](https://github.com/NGO-Algorithm-Audit/Bias_scan).

#### Stanford's AI Audit Challenge 2023

This case study, in combination with our [bias scan tool](https://algorithmaudit.eu/bias_scan), has been selected as a finalist for [Stanford’s AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).

{{< image image="/images/supported_by/HAI.png" alt="Stanford University AI" caption="Stanford University AI" width="8" >}}

#### Presentation

A visual presentation of this case study can be found in this [slide deck](https://github.com/NGO-Algorithm-Audit/Bias_scan/blob/master/Main_presentation_joint_fairness_assessment_method.pdf).

#### Report

Dowload the full report and problem statement [here](https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/view?usp=sharing).

{{< pdf_frame articleUrl="/pdf-files/Case_study_disinfo.pdf" >}}

#### Normative advice commission

* Anne Meuwese, Professor in Public Law & AI at Leiden University
* Hinda Haned, Professor in Responsible Data Science at University of Amsterdam
* Raphaële Xenidis, Associate Professor in EU law at Sciences Po Paris
* Aileen Nielsen, Fellow Law\&Tech at ETH Zürich
* Carlos Hernández-Echevarría, Assistant Director and Head of Public Policy at the anti-disinformation nonprofit fact-checker [Maldita.es](https://maldita.es/maldita-es-journalism-to-not-be-fooled/)
* Ellen Judson, Head of CASM and Sophia Knight, Researcher, CASM at Britain’s leading cross-party think tank [Demos](https://demos.co.uk/)

#### Funded by

{{< image image="/images/supported_by/EUAISFund.png" alt="European Artificial Intelligence & Society Fund" caption="European Artificial Intelligence & Society Fund" width="2" >}}

{{< tab_content_close >}}

{{< tab_content_open icon="" title="" id="actions" >}}

{{< accordions_area_open id="actions" >}}

{{< accordion_item_open title="Finalist selection Stanford's AI Audit Challenge 2023" image="/images/supported_by/HAI.png" id="ai_audit_challenge" date="14-02-2024" tag1="finalist" >}}

##### Description

Our [bias detection tool](https://algorithmaudit.eu/technical-tools/BDT) and this case study have been selected as a finalist for [Stanford’s AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).

{{< pdf_frame articleUrl="/pdf-files/BDT_main_presentation_JFAM.pdf" width="12" >}}

{{< accordion_item_close >}}

{{< accordions_area_close >}}

{{< tab_content_close >}}

{{< form >}}
