---
layout: case
title: Multi-dimensional bias in a BERT-based disinformation classifier
subtitle: |
  Problem statement (ALGO:AA:2023:01:P) and advice document (ALGO:AA:2023:01:A)
image: /images/algoprudence/AA202301/Cover.png
---

{{< tab_header width="6" tab1_id="description" tab1_title="Description of algoprudence" tab2_id="actions" tab2_title="Actions following algoprudence" tab3_id="" tab3_title="" default_tab="description" >}}

{{< tab_content_open icon="fa-newspaper" title="Multi-dimensional bias in a BERT-based disinformation classifier" id="description" >}}

#### Algoprudence identification code

ALGO:AA:2023:01

#### Summary advice

The advice commission believes there is a low risk of (multi-dimensional) proxy discrimination by the BERT-based disinformation classifier and that the particular difference in treatment identified by the quantitative bias scan can be justified, if certain conditions apply.

#### Source of case

Applying our self-build unsupervised [bias detection tool](/technical-tools/bdt/) on a self-trained BERT-based disinformation classifier on the Twitter1516 dataset. Learn more on [Github](https://github.com/NGO-Algorithm-Audit/Bias-detection-tool).

#### Stanford's AI Audit Challenge 2023

This case study, in combination with our [bias detection tool](/technical-tools/bdt/), has been selected as a finalist for [Stanford’s AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).

{{< image id="stanford" width_desktop="6" width_mobile="12" image1="/images/partner logo-cropped/StanfordHAI.png" link1="https://hai.stanford.edu/ai-audit-challenge-2023-finalists" alt1="Stanford University" caption1="Stanford University" >}}

#### Presentation

A visual presentation of this case study can be found in this [slide deck](https://github.com/NGO-Algorithm-Audit/Bias_scan/blob/master/Main_presentation_joint_fairness_assessment_method.pdf).

#### Problem statement and advice document

{{< embed_pdf url2="/pdf-files/algoprudence/ALGO_AA202301/ALGO_AA202301A_Case_study_disinfo.pdf" url="/pdf-files/algoprudence/ALGO_AA202301/ALGO_AA202301P_Case_study_disinfo.pdf" >}}

#### Normative advice commission

* Anne Meuwese, Professor in Public Law & AI at Leiden University
* Hinda Haned, Professor in Responsible Data Science at University of Amsterdam
* Raphaële Xenidis, Associate Professor in EU law at Sciences Po Paris
* Aileen Nielsen, Fellow Law\&Tech at ETH Zürich
* Carlos Hernández-Echevarría, Assistant Director and Head of Public Policy at the anti-disinformation nonprofit fact-checker [Maldita.es](https://maldita.es/maldita-es-journalism-to-not-be-fooled/)
* Ellen Judson, Head of CASM and Sophia Knight, Researcher, CASM at Britain’s leading cross-party think tank [Demos](https://demos.co.uk/)

{{< tab_content_close >}}

{{< tab_content_open icon="" title="" id="actions" >}}

{{< accordions_area_open id="actions" >}}

{{< accordion_item_open image="/images/partner logo-cropped/SIDN.png" title="Funding for further development" id="sidn" date="01-12-2023" tag1="funding" tag2="open source" tag3="AI auditing tool" >}}

##### Description

[SIDN Fund](https://www.sidnfonds.nl/projecten/open-source-ai-auditing) is supporting Algorithm Audit for further development of the bias detection tool. On 01-01-2024, a [team](/nl/about/teams/#bdt) has started that is further developing a testing the tool.

{{< accordion_item_close >}}

{{< accordion_item_open title="Finalist selection Stanford's AI Audit Challenge 2023" image="/images/partner logo-cropped/StanfordHAI.png" id="ai_audit_challenge" date="28-04-2023" tag1="finalist" >}}

##### Description

Our [unsupervised bias detection tool](/technical-tools/bdt/) and this case study have been selected as a finalist for [Stanford’s AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).

{{< accordion_item_close >}}

{{< accordions_area_close >}}

{{< tab_content_close >}}

{{< form1 >}}
