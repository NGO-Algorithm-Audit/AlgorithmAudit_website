---
title: Higher-dimensional bias in a BERT-based disinformation classifier
subtitle: |
  Problem statement (ALGO:AA:2023:01:P) and advice document (ALGO:AA:2023:01:A)
image: /images/algoprudence/AA202301/Cover.png
form1:
  title: React to this normative judgement
  content: >-
    Your reaction will be sent to the team maintaining algoprudence. A team will
    review your response and, if it complies with the guidelines, it will be placed in the Discussion & debate section
    above.
  button_text: Submit
  backend_link: 'https://formspree.io/f/xyyrjyzr'
  id: case-reaction
  questions:
    - label: Name
      id: name
      required: true
      type: text
    - label: Affiliated organization
      id: affiliated-organization
      type: text
    - label: Reaction
      id: reaction
      required: true
      type: textarea
    - label: Contact detail
      id: contact-details
      required: true
      type: email
      placeholder: Mail address
layout: case
icon: fa-newspaper
summary: >
  The advice commission believes there is a low risk of (higher-dimensional)
  proxy discrimination by the BERT-based disinformation classifier and that the
  particular difference in treatment identified by the quantitative bias scan
  can be justified, if certain conditions apply.
sources: "Applying our self-build unsupervised\_[bias detection tool](https://algorithmaudit.eu/bias_scan)\_on a self-trained BERT-based disinformation classifier on the Twitter1516 dataset. Learn more on\_[Github](https://github.com/NGO-Algorithm-Audit/Bias_scan).\n"
additional_content:
  - title: Stanford's AI Audit Challenge 2023
    content: "This case study, in combination with our\_[bias scan tool](https://algorithmaudit.eu/bias_scan), has been selected as a finalist for\_[Stanford's AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).\n"
    image: /images/supported_by/HAI.png
    image_link: 'https://hai.stanford.edu/ai-audit-challenge-2023-finalists'
    width: 8
  - title: Presentation
    content: "A visual presentation of this case study can be found in this\_[slide deck](https://github.com/NGO-Algorithm-Audit/Bias_scan/blob/master/Main_presentation_joint_fairness_assessment_method.pdf).\n"
    width: 12
algoprudence:
  title: Report
  intro: "Dowload the full report and problem statement\_[here](https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/view?usp=sharing).\n"
  reports:
    - url: >-
        https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/preview
normative_advice_members:
  - name: |
      Anne Meuwese, Professor in Public Law & AI at Leiden University
  - name: >
      Hinda Haned, Professor in Responsible Data Science at University of
      Amsterdam
  - name: |
      Raphaële Xenidis, Associate Professor in EU law at Sciences Po Paris
  - name: |
      Aileen Nielsen, Fellow Law\&Tech at ETH Zürich
  - name: "Carlos Hernández-Echevarría, Assistant Director and Head of Public Policy at the anti-disinformation nonprofit fact-checker\_[Maldita.es](https://maldita.es/maldita-es-journalism-to-not-be-fooled/)\n"
  - name: "Ellen Judson, Head of CASM and Sophia Knight, Researcher, CASM at Britain’s leading cross-party think tank\_[Demos](https://demos.co.uk/)\n"
funded_by:
  - url: 'https://europeanaifund.org/'
    image: /images/supported_by/EUAISFund.png
actions:
  - id: ai_audit_2023
    title: Finalist selection Stanford's AI Audit Challenge 2023
    description: >
      Our [bias detection tool](/technical-tools/BDT) and this case study have
      been selected as a finalist for [Stanford's AI Audit Challenge
      2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).


      {{< pdf_frame >}}
    image: /images/supported_by/HAI.png
    date: 14-02-2024
    facets:
      - value: finalist
        label: finalist
---

{{< tab_header width="6" tab1_id="description" tab1_title="Description of algoprudence" tab2_id="actions" tab2_title="Actions following algoprudence" tab3_id="" tab3_title="" default_tab="description" >}}

{{< tab_content_open icon="fa-newspaper" title="Higher-dimensional bias in a BERT-based disinformation classifier" id="description" >}}

#### Algoprudence identification code

ALGO:AA:2023:01

#### Summary advice

The advice commission believes there is a low risk of (higher-dimensional) proxy discrimination by the BERT-based disinformation classifier and that the particular difference in treatment identified by the quantitative bias scan can be justified, if certain conditions apply.

#### Source of case

Applying our self-build unsupervised [bias detection tool](/technical-tools/bdt/) on a self-trained BERT-based disinformation classifier on the Twitter1516 dataset. Learn more on [Github](https://github.com/NGO-Algorithm-Audit/Bias-detection-tool).

#### Stanford's AI Audit Challenge 2023

This case study, in combination with our [bias detection tool](/technical-tools/bdt/), has been selected as a finalist for [Stanford’s AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).

{{< image id="stanford" width_desktop="6" width_mobile="12" image1="/images/supported_by/HAI.png" link1="https://hai.stanford.edu/ai-audit-challenge-2023-finalists" alt1="Stanford University" caption1="Stanford University" >}}

#### Presentation

A visual presentation of this case study can be found in this [slide deck](https://github.com/NGO-Algorithm-Audit/Bias_scan/blob/master/Main_presentation_joint_fairness_assessment_method.pdf).

#### Report

Dowload the full report and problem statement [here](https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/view?usp=sharing).

{{< pdf_frame articleUrl1="https://drive.google.com/file/d/1GHPwDaal3oBJZluFYVR59e1_LHhP8kni/preview" articleUrl2="" >}}

#### Normative advice commission

* Anne Meuwese, Professor in Public Law & AI at Leiden University
* Hinda Haned, Professor in Responsible Data Science at University of Amsterdam
* Raphaële Xenidis, Associate Professor in EU law at Sciences Po Paris
* Aileen Nielsen, Fellow Law\&Tech at ETH Zürich
* Carlos Hernández-Echevarría, Assistant Director and Head of Public Policy at the anti-disinformation nonprofit fact-checker [Maldita.es](https://maldita.es/maldita-es-journalism-to-not-be-fooled/)
* Ellen Judson, Head of CASM and Sophia Knight, Researcher, CASM at Britain’s leading cross-party think tank [Demos](https://demos.co.uk/)

#### Funded by

<br>

{{< image id="funded-by" width_desktop="4" width_mobile="6" image1="/images/supported_by/EUAISFund.png" alt1="European Artificial Intelligence & Society Fund" caption1="European Artificial Intelligence & Society Fund" link1="https://europeanaifund.org/" >}}

{{< tab_content_close >}}

{{< tab_content_open icon="" title="" id="actions" >}}

{{< accordions_area_open id="actions" >}}

{{< accordion_item_open image="/images/supported_by/sidn.png" title="Funding for further development" id="sidn" date="01-12-2023" tag1="funding" tag2="open source" tag3="AI auditing tool" >}}

##### Description

[SIDN Fund](https://www.sidnfonds.nl/projecten/open-source-ai-auditing) is supporting Algorithm Audit for further development of the bias detection tool. On 01-01-2024, a [team](/nl/about/teams/#bdt) has started that is further developing a testing the tool.

{{< accordion_item_close >}}

{{< accordion_item_open title="Finalist selection Stanford's AI Audit Challenge 2023" image="/images/supported_by/HAI.png" id="ai_audit_challenge" date="28-04-2023" tag1="finalist" >}}

##### Description

Our [bias detection tool](https://algorithmaudit.eu/technical-tools/BDT) and this case study have been selected as a finalist for [Stanford’s AI Audit Challenge 2023](https://hai.stanford.edu/ai-audit-challenge-2023-finalists).

{{< accordion_item_close >}}

{{< accordions_area_close >}}

{{< tab_content_close >}}

{{< form1 >}}
