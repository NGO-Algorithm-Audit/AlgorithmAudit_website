---
title: Synethetic Data Generation
subtitle: >
  Synthetic Data Generation (SDG) provides a privacy-preserving alternative for
  training and debiasing algorithmic systems. At Algorithm Audit, we build
  public knowledge how SDG can be used best for AI bias testing.
image: /images/svg-illustrations/knowledge_base.svg
author: Parie
type: regular
team:
  title: Synthetic Data Generation Team
  icon: fas fa-user-friends
  button_text: Other teams
  id: team
  button_link: /about
  team_members:
    - image: /images/people/EMenvouta.jpeg
      name: Emmanuel Menvouta PhD
      bio: |
        Machine Learning Engineer, Dataroots
    - image: /images/people/EBogaards.jpeg
      name: Ellen Bogaards
      bio: |
        MSc Artifical Intelligence, Utrecht University
    - image: /images/people/GAcheampong.jpeg
      name: Godwin Acheampong
      bio: |
        Data Scientist, Budget Thuis
    - image: /images/people/SBabac.jpeg
      name: Sonja Babac
      bio: |
        PhD-candidate, Technical University Eindhoven – Philips MedTech
    - image: /images/people/JPersson.jpeg
      name: Joel Persson PhD
      bio: |
        Research Scientist, Spotify
text_field1:
  title: Synthetic Data Generation – What is it?
  icon: fas fa-database
  id: info
  content: >
    Garbage in, garbage out: When auditing sADM semi-automated decision-making
    processes, one of the most immediate questions is the representativeness of
    the source data. However, privacy poses a hurdle to sharing data with
    external parties for bias testing. Absent access to source data, external
    auditors cannot assess bias, limiting trust in the data sets used. Synthetic
    data generation (SDG) – artificial datasets mimicking the statistical
    characteristics of the original dataset – is a potential solution for this
    issue. It is considered a safe approach for the wider release of privately
    held data, as it contains no identifiable trace of the personal data it was
    generated from.
text_field2:
  title: How can SDG be used for AI bias testing?
  icon: fas fa-cog
  id: methods
  content: >
    SDG holds potential for third parties to audit datasets in a
    privacy-preserving way. There is currently not yet sufficient knowledge how
    and when SDG serves as a suitable method for external bias testing. First of
    all, the complex process of SDG may not always be necessary for bias testing
    when, for instance, univariate or bivariate aggregation statistics suffice.
    Second, there are various technical ways to conduct SDG (e.g., parametric,
    non-parametric, copula-based), the most suitable of which is not evident. At
    Algorithm Audit, we are investigating these open questions, and build public
    knowledge on what form of data-sharing practice (SDG or alternatives) is
    best suited for privacy-preserving AI bias testing.
---

{{< text_field1 >}}

{{< text_field2 >}}

{{< team title="Hoer" >}}
