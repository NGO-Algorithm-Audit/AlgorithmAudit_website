---
title: Press releases
subtitle: >
  Press releases of Algorithm Audit. Get in touch with the organisation by
  submitting [this](/about/contact/) contact form.
image: /images/svg-illustrations/about.svg
quick_navigation:
  title: Overview
  links:
    - title: Brouwer Trust Prize
      url: '#KHMW'
    - title: DUO2 Addendum Preventing prejudice
      url: '#DUO_CBS'
    - title: DUO1 Preventing prejudice
      url: '#DUO'
    - title: Onderzoek Rotterdam
      url: '#Rotterdam'
---

<!-- KHMW -->

{{< accordions_area_open id="KHWM">}}

{{< accordion_item_open title="Algorithm Audit wins Brouwer Trust Prize of €100.000" id="KHMW" background_color="#ffffff" date="19-01-2026" tag1="KHMW" tag2="Brouwer Trust Prize" image="/images/partner logo-cropped/KHMW.png" >}}

<b>THE HAGUE – Algorithm Audit is celebrating: on Monday 19 January we received the first prize of the KHMW Brouwer Trust Prizes 2026. This prize is an important milestone for our foundation and a recognition of the societal relevance of our work.</b>

With the <a href="https://khmw.nl" target="_blank">Brouwer Trust Prizes</a>, the Royal Holland Society of Sciences and Humanities (KHMW) supports societal initiatives that strengthen mutual trust in Dutch society. This year, the first prize (€100.000) goes to stichting Algorithm Audit. Thanks in part to this prize, we can conduct more case studies in the coming years and thus contribute to the responsible deployment of AI. Director Jurriaan Parie in an <a href="https://khmw.nl/winnaar-eerste-prijs-khmw-brouwer-vertrouwensprijzen-2026-algorithm-audit/" target="_blank">interview</a> with KHMW: “Having agency in how technology works gives people a sense of control, and that control cultivates trust. Thanks to the Brouwer Trust Prize, we can continue to pursue our societal ambitions”.

#### Laudatio by the jury
During the award ceremony of the Trust Prize in Haarlem, the jury emphasized that “Algorithm Audit addresses a highly topical issue and provides an important counterweight to the rising power of tech companies and AI, as well as to the unequal balance of power that often exists between government and citizens.” The jury expressed the hope “that the increasing visibility and strength of Algorithm Audit will also lead to a more self-regulating attitude among the relevant companies and institutions, who, by critically evaluating their own algorithms and adjusting them where necessary, will become more credible partners for citizens and customers.”

#### Having a say in the deployment of algorithms and AI
Winning this prize feels like the crowning achievement of our work over the past years. Since its founding in 2021, Algorithm Audit has worked to give citizens and stakeholders a voice in how algorithms and AI are deployed. The foundation convenes independent advice commissions in which academic experts and citizen representatives jointly assess concrete AI applications. 

This approach, which we call <i>algoprudence</i>, ensures that society can help shape the future of technology in a transparent and democratic way. Jurriaan Parie: “We want to give citizens and civil society a voice in how technology is deployed, by governments and companies alike. Technology is increasingly shaping our daily lives. The question is: how much agency do we have over it, and who gets to have a say?”
In recent years, Algorithm Audit has examined multiple risk-profiling algorithms at Dutch municipalities, at the Education Executive Agency (DUO), and at a car-sharing platform. This led to concrete recommendations to governmental actors, public accountability and improvements in policy measures and deployment of algorithmic system. The tools we develop to investigate data-driven applications are made available open source so that anyone can use them to, for example, detect anomalies.

#### Our plans
With the prize money we can strengthen the organization and expand the scope of our work. In the coming years we plan to delve further into generative AI. As chatbots increasingly mediate between government and citizens, and between consumers and companies, risks arise. New evaluation methods are needed: do these information systems provide reliable information, are they actually effective, and do chatbots appropriately refer users to human help? 

In the near future we will identify pressing issues and develop case studies. Our algoprudence adds societal value by capturing and publicly sharing the voices of a diverse group of stakeholders. By sharing knowledge and bringing stakeholders together, we aim to contribute to a society in which technology strengthens, rather than undermines, mutual trust.

#### More about Algorithm Audit
Algorithm Audit is a knowledge platform for responsible AI. The not-for-profit organisation brings together expertise at the intersection of technology, law and ethics. As a growing national and European knowledge platform, we share our expertise through research, white papers, workshops, and lectures.

{{< accordion_item_close >}}


<!-- DUO 2 -->

{{< accordion_item_open title="DUO control process biased towards students with a non-European migration background" id="DUO_CBS" background_color="#ffffff" date="22-05-2024" tag1="DUO" tag2="CBS" tag3="bias analysis" image="/images/partner logo-cropped/DUO.png" >}}

**THE HAGUE - In its inspection of the legitimate use of student finance for students living away from home, DUO selected students for control with a non-European migration background significantly more often. This demonstrates an unconscious bias in DUO's control process. Students with a non-European migration background were assigned a higher risk score by a risk profile and were more often manually selected for a home visit. This is evident from follow-up research that NGO Algorithm Audit carried out on behalf of DUO, which was sent by the minister to the House of Representatives on May 22. The results of the research strengthen the outcomes of previous research, on the basis of which the minister apologized on behalf of the cabinet on March 1, 2024 for indirect discrimination in the control process.**

Detailed data provided by CBS on the origin of more than 300,000 students in the period 2014-2022 shows that the bias arises in various steps of the control process. Both due to the risk profile that was used and the subsequent manual selection, students with a non-European migration background were more often selected for control. Even though the origin of students did not play a direct role in DUO's control process, an indirect bias arises as a result of the selection criteria.

Certain characteristics (including the type of education mbo 1-2 and a short distance to the parental address) led to an increased risk score. And because these features are more common among students with a non-European migration background, it resulted in overrepresentation of this group. Other characteristics also played a role in the decision regarding whether a student was selected for control, including home registration with family or in a student house. The effect of all these factors posed a magnifying glass effect, which allowed DUO to detect unduly use, especially among students with a non-European migration background. Unduly use of the college grant by students without a migration background more often remained out of the picture.

Data analysis of Algorithm Audit shows that students with a non-European migration background were seen as high risk by the risk profile 2.0x more often than students without a migration background. The group was manually selected 6.2x more often for a home visit. Ultimately, students with a non-European migration background had a 3.0x greater chance of being visited at home unjustly than students with a Dutch origin.

The research demonstrates a consistent image for the period 2014-2022 that the group of students who appealed against a DUO decision mainly consists of students with a non-European migration background. This confirms the notion from [reports](https://nos.nl/op3/video/2479701-zo-checkt-duo-of-jij-fraudeert-en-dat-systeem-rammelt) by Investico and NOS in 2023, that students with a migration background were accused more often than other students of unduly using student finance and who appealed as a result. Previous research by PwC revealed, based on postal code statistics, that students registered in migrant neighborhoods were selected for control more often by DUO. DUO requested Algorithm Audit for follow-up research based on detailed data from Statistics Netherlands (CBS). This data makes it possible to measure exactly where and how much bias occurs in the control process. The results of Algorithm Audit's follow-up study reveals an amplified bias compared to previous studies. Both the CBS data and the research methods used by Algorithm Audit are publicly accessible (can be found [here](https://www.cbs.nl/nl-nl/maatwerk/2024/21/ontvangers-uitwonendenbeurs-herkomst-2014-2017-2019-2021-en-2022) and here).

DUO has currently ceased the standard control process for the college grant and solely selects students at random. DUO and Algorithm Audit continue to work together to interpret the results of the studies together by engaging various social stakeholders, as well as considering improvements in a possible future audit process.

The Bias Prevented report (addendum) can be found [here](https://algorithmaudit.eu/algoprudence/cases/aa202402_bias-prevented_addendum/).

{{< accordion_item_close >}}

<!-- DUO 1 -->

{{< accordion_item_open title="Irregularities identified in college allowances control process by Dutch public sector organization DUO" id="DUO" background_color="#ffffff" date="01-03-2024" tag1="DUO" tag2="audit report" tag3="" image="/images/partner logo-cropped/DUO.png" >}}

**THE HAGUE – In its control process into misuse of the allowances for students living away from home, Dutch public sector organization DUO selected individuals who lived close to their parent(s) significantly more often. The risk-taxation algorithm, that was used as an assisting tool for selecting students, worked as expected. However, the combination of the algorithm and manual selection resulted in a large overrepresentation of certain groups. Selected students were visited at home to inspect whether allowances were unduly granted. This is the main conclusion of research carried out by NGO Algorithm Audit on behalf of DUO. DUO’s control process was discredited in 2023 after media attention, in which was mentioned that students with a migration background were accused of misuse more often than other students.**

Students who were registered living close to their parent(s) were significantly more likely to be selected for a home visit. As a result of such investigations, DUO decides whether the allowances for people living away from home has been frauded with. Such a deviation would not have occurred if the output of a rule-based algorithm had been followed. This algorithm was used by the Dutch Education Executive Agency (‘DUO’) to assign a risk score to all students (more than 500,000) living away from their parent(s) in the period 2012-2022. It is obvious that specific work instructions – which encourage the manual selection of students who are registered near the parental address – have resulted in this disparity. That is the main conclusion of the study Bias prevented of NGO Algorithm Audit, which was sent to the Dutch Parliament on March 1, 2024.

Furthermore, the study does not prove statistical evidence between multiple selection criteria as used in the risk assessment algorithm (i.e., type of education and age) and unduly granted allowances. For the selection criterion distance to parent(s), statistical evidence for such a link with has been found. These results are based on a statistical analysis of risk distributions in random samples drawn from the student population in 2014 and 2017. NGO Algorithm Audit also found that the algorithm does adhere to current standards set for usage of algorithms used in decision-making processes by the Dutch government. At the request of DUO, standards from 2023 were used to assess its decision-making process which has been deployed in the period 2012-2014. For instance, no rationale has been documented why important choices were made during the design phase of the algorithm. Furthermore, the control process has not undergone scrutiny for indirect bias, neither during the design phase nor during the deployment phase.

Commissioned by DUO, NGO Algorithm Audit is auditing the control process of college allowances. The reason for this is that students with a migration background are more often accused of misusage of allowances. Whether a relationship exists between the deviating group, that has been selected significantly more often for a home visit, and students with a migration background will become clear from further research that is currently being undertaken. Data requested from Statistics Netherlands (CBS) will be analyzed to measure the percentage of students per country of origin for each step of the allowances control process. According to NGO Algorithm Audit, publicly accessible data – such as aggregated migration statistics per ZIP code area or the average distance that certain demographic groups live from their parent(s) – is too inaccurate to be used for this delicate research. According to NGO Algorithm Audit, it is too early to conclude whether students with a migration background are actually being disadvantaged, although the study has not demonstrated the opposite. NGO Algorithm Audit does note that there is no evidence of direct discrimination on the basis of migration background in the algorithm.

The results of the studies will be used to determine whether risk profiling can be used responsibly in the future to detect misuse of college allowances. This is relevant as these allowances have been reintroduced since this academic year for students following a higher vocational study (hbo) or a scientific/academic study (wo). DUO and NGO Algorithm Audit will continue to work together in 2024 to interpret the results of the studies together with various societal stakeholders. Currently, DUO solely randomly selects students for checking cases of fraud.

The full report Bias prevented can be found [here](https://algorithmaudit.eu/algoprudence/cases/aa202401_bias-prevented/).

{{< accordion_item_close >}}

<!-- Rotterdam -->

{{< accordion_item_open title="Independent commission publishes advice to Dutch municipalities on risk profiling for social welfare re-examination" id="Rotterdam" background_color="#ffffff" date="29-11-2023" image="/images/logo/logo.svg" tag1="municipality of Rotterdam" tag2="algoprudence" tag3="machine learning" >}}

**THE HAGUE – On November 29, Dutch Minister of Digitalisation Alexandra van Huffelen accepted the advice report containing specific norms for the responsible utilization of algorithms. Algorithm Audit convened an independent commission of experts to provide advice aimed at preventing unfair treatment in social welfare re-examinations. The impetus for drafting this advice was the controversial risk model used by the Municipality of Rotterdam until 2021.**

When municipalities investigate whether recipients of social assistance is duly granted, algorithmic profiling is sometimes employed. The advisory report, prepared by representatives of the Ombudsman of Amsterdam and Rotterdam, various academics, and an alderman from Tilburg, advises against using characteristics such as ZIP code, the number of children, and literacy rate as criteria for profiling. It has been previously demonstrated that profiling in the context of social assistance can lead to discrimination. The independent commission asserts that there are also other ethical concerns associated with algorithmic profiling, as some self-learning algorithms are too complex to effectively explain decisions to citizens. The commission recommends discontinuing the use of certain machine learning algorithms.

The advice is directed at all 340 Dutch municipalities. The guidance is part of the ‘algoprudence’ developed by Algorithm Audit in collaboration with experts and various interest groups to establish concrete norms for the responsible deployment of algorithms. NGO Algorithm Audit is an independent knowledge platform for ethical algorithms and AI, supported by the European AI\&Society Fund, the SIDN Fund, and the Ministry of the Interior and Kingdom Relations.

The full report Risk profiling for Social Welfare Re-examination can be found [here](https://algorithmaudit.eu/algoprudence/cases/aa202302_risk-profiling-for-social-welfare-reexamination/).

{{< image id="presentatie" width_desktop="6" width_mobile="12" image1="/images/algoprudence/AA202302/Algorithm audit presentatie BZK FB-18.jpg" alt1="Presentatie Staatssecretaris voor Digitalisering" caption1="Presentatie Staatssecretaris voor Digitalisering" >}}

29-11-2023

{{< container_close >}}
