---
title: AI Policy Observatory
subtitle: >
  AI ethics urgently needs case-based experience and a bottom-up approach. We
  believe existing and proposed legislation is and will not suffice to realize
  ethical algorithms. For various policy initiatives, we elaborate below why.
image: /images/svg-illustrations/knowledge_base.svg
background_color: '#ffffff'
Why_we_exist:
  enable: true
  new_items:
    - title: Why we exist
      subtitle: AI policy observatory
      tab1: AI Act
      tab2: DSA
      tab3: GDPR
      tab4: Administrative law
      tab5: HRIA
      tab6: AI registers
      tab7: Regulatory landscape
      tab8: Adminis-trative law
      tab9: AI registers
      intro_content: >-
        AI ethics urgently needs case-based experience and a bottom-up approach.
        We believe existing and proposed legislation is and will not suffice to
        realize ethical algorithms. Why not?
      ai_act_title: AI Act
      ai_act_content: >-
        The <a
        href="https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1623335154975&uri=CELEX%3A52021PC0206"
        target="_blank">AI Act </a> imposes broad new responsibilities to
        control risks from AI systems without at the same time laying down
        specific standards they are expected to meet. For instance<span>:</span>
      ai_act_tag_conformity: 'Conformity assessment (Art. 43)<span>:</span>'
      ai_act_content_conformity: >-
        The proposed route for internal control relies too much on the
        self-reflective capacities of producers to assess AI quality management,
        risk management and bias. Resulting in subjective best-practices;
      ai_act_tag_risk: 'Risk- and quality management systems (Art. 9 and 17)<span>:</span>'
      ai_act_content_risk: >-
        Requirements set out for risk management systems and quality management
        systems remain too generic. For example, it does not provide precise
        guidelines how to identify and mitigate ethical issues such as
        algorithmic discrimination;
      ai_act_tag_standards: 'Normative standards<span>:</span>'
      ai_act_content_standards: >-
        To realize AI harmonization across the EU, publicly available technical
        and normative best-practices for fair AI are urgently needed.
      ai_act_white_paper: Read our algoprudence on proxy discrimination
      ai_act_image: /images/AIA.png
      dsa_title: Digital Service Act (DSA)
      dsa_content: >-
        The <a
        href="https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52020PC0825"
        target="_blank">DSA</a> lacks provisions to disclose normative
        methodological choices that underlie general purpose AI systems. For
        instance<span>:</span>
      dsa_tag_risk: 'Risk definitions<span>:</span>'
      dsa_content_risk: >-
        Article 9 of the <a
        href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits_en"
        target="_blank">Delegated Regulation (DR)</a> for independent third
        party auditing (as mandated under DSA Art. 37) specifies that “audit
        risk analysis shall consider _inherent risk_, _control risk_ and
        _detection risk_”. More specific guidance should be provided in Art. 2
        of the DR how risks relating to subjective concepts, such as “...the
        nature, the activity and the use of the audited service”, can be
        assessed;
      dsa_tag_template: 'Audit template<span>:</span>'
      dsa_content_template: >-
        Pursuant to Article 5(1)(a) of the DR, Very Large Open Platforms (VLOPs)
        and Very Large Online Search Engines (VLOSEs) shall transmit to
        third-party auditing organisations “benchmarks used [...] to assert or
        monitor compliance [...], as well as supporting documentation”. We argue
        that the normative considerations underlying the selection of these
        benchmarks should be asked out more decisively in this phase of the
        audit. Therefore, we asked the European Commission (EC) to add this
        dimension to Question 3(a) of Section D.1 _Audit conclusion for
        obligation Subsection_ II. _Audit procedures and their results_;
      dsa_white_paper: >-
        Read our feedback to the Europen Commission on DSA Art. 37 Delegated
        Regulation
      dsa_content_feedback: >-
        <a
        href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits/feedback_en?p_id=32081201"
        target="_blank">Feedback</a> submitted to the European Commission (EC)
        on DSA Art. 37 DR showcasts that<span>:</span>
      dsa_content_feedback_tab1: >-
        Private auditors (like PwC and Deloitte) warn that the lack of guidance
        on criteria against which to audit poses a risk of subjective audits;
      dsa_content_feedback_tab2: >-
        Tech companies (like Snap and Wikipedia) raise concerns about the
        industry's lack of expertise to audit specific AI products, like
        company-tailored timeline recommender systems.
      dsa_image: /images/DSA.jpg
      gdpr_title: General Data Protection Regulation (GDPR)
      gdpr_content1: >-
        Organizations that develop algorithms do often not comply with GDPR
        provisions that foster participatory algorithm development. For
        example<span>:</span>
      gdpr_content2: >-
        Besides, the current regulation only partially specifies measures to
        safeguard algorithmic decision-making. For instance<span>:</span>
      gdpr_tag_dpia: 'Participatroy DPIA (art. 35 sub 9)<span>:</span>'
      gdpr_content_dpia: >-
        This provision mandates that in cases where a Data Privacy Impact
        Assessment (DPIA) is obligatory, the opinions of data subjects regarding
        the planned data processing shall be seeked. This is a powerful legal
        mechanism to foster collaborative algorithm development. Nevertheless,
        the inclusion of data subjects in this manner is scarcely observed in
        practice;
      gdpr_tag_profiling: Profiling (recital 71)
      gdpr_content_profiling: >-
        is broadly defined as<span>:</span> “to analyse or predict aspects
        concerning the data subject’s performance at work, economic situation,
        health, personal preferences or interests, reliability or behaviour,
        location or movements”. However, the approval of profiling, particularly
        when “authorised by Union or Member State law to which the controller is
        subject, including fraud monitoring”, grants public and private entities
        significant flexibility to integrate algorithmic decision-making derived
        from diverse types of profiling. This wide latitude raises concerns
        about the potential for excessive consolidation of personal data and the
        consequences of algorithmic determinations;
      gdpr_tag_ADM: 'Automated decision-making (art. 22 sub 2)<span>:</span>'
      gdpr_content_ADM: >-
        Allowing wide-ranging automated decision-making (ADM) and profiling
        under the sole condition of contract agreement opens the door for large
        scale unethical algorithmic practices without accountability and public
        awareness.
      gdpr_image: /images/gdpr.svg
      administrative_law_title: Administrative law
      administrative_law_content: >-
        Unifying principles of sound administration with (semi-) automated
        decision-making is challenging. For instance<span>:</span>
      administrative_law_tag_motivation: 'Obligation to state reasons:</span>'
      administrative_law_content_motivation: >-
        Governmental institutions must always provide clear explanations for
        their decisions. However, when machine learning is employed, such as in
        variable selection for risk profiling, this transparency may be
        obscured. This leads to the question of how far arguments based on
        probability distributions are acceptable as explanations for why certain
        citizens are chosen for a particular profile.
      algoprudence_rotterdam: >-
        Read Algoprudence AA:2023:02 for a review of xgboost machine learning
        used for risk profiling variable selection
      administrative_law_image: /images/Awb.jpg
      regulator_title: Regulatory landscape
      regulator_content: >-
        <a
        href="https://www.rekenkamer.nl/onderwerpen/algoritmes-digitaal-toetsingskader/ethiek"
        target="_blank">Perspective 3.1.1</a> in the Guidelines for Algorithms
        of the Dutch Court of Auditors argues that ethical algorithms are not
        allowed to “discriminate and that bias should be minimised”. Missing
        from this judgment is a discussion of what precisely constitutes bias in
        the context of algorithms and what would be appropriate methods to
        ascertain and mitigate algorithmic discrimination. In the absence of a
        clear ethical framework, it is up to organizations to formulate
        context-sensitive approaches to combat discrimination.
      regulator_image: /images/FRIA.png
      hria_title: Fundamental Rights and Human Rights Impact Assessments
      hria_content: >-
        The <a
        href="https://www.rijksoverheid.nl/documenten/rapporten/2021/02/25/impact-assessment-mensenrechten-en-algoritmes"
        target="_blank">Impact Assessment Human Rights and Algorithms (IAMA)</a>
        and the <a
        href="https://www.rijksoverheid.nl/documenten/rapporten/2021/06/10/handreiking-non-discriminatie-by-design"
        target="_blank">Handbook for Non-Discrimination</a>, both developed by
        the Dutch government, assess discriminatory practice mainly by asking
        questions that are meant to stimulate self-reflection. It does not
        provide answers or concrete guidelines how to realise ethical
        algorithms.
      hria_image: /images/FRIA.png
      AI_register_title: AI registers
      AI_register_content: ..
      AI_register_image: /images/knowledge_base.svg
      content1: >-
        We believe a case-based and context sensitive approach is indispensable
        to develop ethical algorithms. One should not expect top-down regulation
        and legislation to solve all ethical problems in AI and machine
        learning. Taking all contested algorithmic cases to court is practically
        infeasible. More importantly, organizations will always carry their own
        responsibility for ethical algorithms within and beyond the obligation
        of legal compliance. Hence, new bottom-up initiatives like Algorithm
        Audit are necessary to support these organizations and to strengthen
        ethical practice in ADM and decision support.


        We provide a platform where experts in AI ethics from various
        disciplines can interact with, learn from and steer actual algorithmic
        practice and surrounding ethical concerns. We increase public knowledge
        and stimulate an informed and open debate about what ethical algorithms
        we desire as a society in various contexts. Our audit commissions shape
        future public values through discussion and deliberation. As such,
        Algorithm Audit contributes in the digital realm to SDG16 – Peace,
        Justice and Strong Institutions.
reports_preview:
  title: White papers
  icon: fas fa-file
  button_text: Read our white papers
  button_link: >-
    /knowledge-platform/knowledge-base/white_paper_dsa_delegated_regulation_feedback/
  id: white-papers
  feature_item:
    - name: Feedback on DSA Delegated Regulation (conducting independent audits)
      image: /images/knowledge_base/white-paper-3.png
      link: >-
        /knowledge-platform/knowledge-base/white_paper_dsa_delegated_regulation_feedback/
      content: >
        Feedback to the European Commission on DSA Delegated Regulation –
        conducting independent audits
---

{{< tab_header width="2" default_tab="administrative-law" tab1_id="AI-Act" tab1_title="AI Act" tab2_id="DSA" tab2_title="DSA" tab3_id="GDPR" tab3_title="GDPR" tab4_id="administrative-law" tab4_title="Administrative law" tab5_id="FRIA" tab5_title="FRIA" tab6_id="algorithm-registers" tab6_title="Registers" >}}

{{< tab_content_open id="AI-Act" icon="fa-newspaper" title="AI Act" >}}

The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1623335154975&uri=CELEX%3A52021PC0206" target="_blank">AI Act</a> imposes broad new responsibilities to control risks from AI systems without at the same time laying down specific standards they are expected to meet. For instance:

* **Conformity assessment (Art. 43) –** The proposed route for internal control relies too much on the self-reflective capacities of producers to assess AI quality management, risk management and bias. Resulting in subjective best-practices;
* **Risk- and quality management systems (Art. 9 and 17) –** Requirements set out for risk management systems and quality management systems remain too generic. For example, it does not provide precise guidelines how to identify and mitigate ethical issues such as algorithmic discrimination;
* **Normative standards –** Technical standards alone, as requested the European Commission to standardization bodies CEN-CENELEC, are not enough to realize AI harmonization across the EU. Publicly available technical and normative best-practices for fair AI are urgently needed.

As a member of Dutch standardization body NEN, Algorithm Audit contributes to the European debate how fundamental rights should be co-regulated by product safety.

{{< button button_text="Learn more about our standardization activities" button_link="/knowledge-platform/standards/" >}}

Our audits take in mind upcoming harmonized standards that will be applicable under the AI Act, excluding  cybersecurity specifications. For each of our technical and normative audit reports is elaborated how it aligns with the current status of AI Act harmonized standards.

{{< button button_text="Case repository" button_link="/algoprudence/" >}}

{{< tab_content_close >}}

{{< tab_content_open icon="fa-newspaper" title="Digital Services Act (DSA)" id="DSA" >}}

The [Digital Services Act (DSA)](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52020PC0825) lacks provisions to disclose normative methodological choices that underlie the AI systems the DSA tries to regulate. For instance:

* **Risk definitions –** Article 9 of the <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits_en" target="_blank">Delegated Regulation (DR)</a> for independent third party auditing (as mandated under DSA Art. 37) specifies that “audit risk analysis shall consider inherent risk, control risk and detection risk”. More specific guidance should be provided in Art. 2 of the DR how risks relating to subjective concepts, such as “…the nature, the activity and the use of the audited service”, can be assessed;
* **Audit templates –** Pursuant to Article 5(1)(a) of the DR, Very Large Open Platforms (VLOPs) and Very Large Online Search Engines (VLOSEs) shall transmit to third-party auditing organisations “benchmarks used \[…] to assert or monitor compliance \[…], as well as supporting documentation”. We argue that the normative considerations underlying the selection of these benchmarks should be asked out more decisively in this phase of the audit. Therefore, we asked the European Commission (EC) to add this dimension to Question 3(a) of Section D.1 Audit conclusion for obligation Subsection II. Audit procedures and their results;
* **Insufficient knowledge how to audit AI –** <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits/feedback_en?p_id=32081201" target="_blank">Feedback</a> submitted to the European Commission (EC) on DSA Art. 37 DR reveals that:
  * Private auditors (like PwC and Deloitte) warn that the lack of guidance on criteria against which to audit poses a risk of subjective audits;
  * Tech companies (like Snap and Wikipedia) raise concerns about the industry’s lack of expertise to audit specific AI products, like company-tailored timeline recommender systems.

#### Read our feedback to the Europen Commission on DSA Art. 37 Delegated Regulation

{{< image id="feedback" width_desktop="4" width_mobile="6" image1="/images/knowledge_base/white-paper-3.png" alt1="Feedback focument" caption1="Feedback focument" link1="/knowledge-platform/knowledge-base/white_paper_dsa_delegated_regulation_feedback/" >}}

{{< button button_text="Read the white paper" button_link="/knowledge-platform/knowledge-base/white_paper_dsa_delegated_regulation_feedback/" >}}

{{< tab_content_close >}}

{{< tab_content_open icon="fa-newspaper" title="General Data Protection Regulation (GDPR)" id="GDPR" >}}

The GDPR has its strengths regarding participatory decision-making, but it has also weaknesses in regulating profiling algorithms and its focus on fully automated decision-making.

* [Participatory DPIA (art. 35 sub 9)](https://gdpr-info.eu/art-35-gdpr/) – This provision mandates that in cases where a Data Privacy Impact Assessment (DPIA) is obligatory, the opinions of data subjects regarding the planned data processing shall be seeked. This is a powerful legal mechanism to foster collaborative algorithm development. Nevertheless, the inclusion of data subjects in this manner is scarcely observed in practice;
* [Profiling (recital 71)](https://gdpr-info.eu/recitals/no-71/) – Profiling is defined as: “to analyse or predict aspects concerning the data subject’s performance at work, economic situation, health, personal preferences or interests, reliability or behaviour, location or movements”. However, the approval of profiling, particularly when “authorised by Union or Member State law to which the controller is subject, including fraud monitoring”, grants public and private entities significant flexibility to integrate algorithmic decision-making derived from diverse types of profiling. This wide latitude raises concerns about the potential for excessive consolidation of personal data and the consequences of algorithmic determinations. As illustrated by simple, rule-based but harmful profiling algorithms in The Netherlands;
* [Automated decision-making (art. 22 sub 2): ](https://gdpr-info.eu/art-22-gdpr/)Allowing wide-ranging automated decision-making (ADM) and profiling under the sole condition of contract agreement opens the door for large scale unethical algorithmic practices without accountability and public awareness.

#### Read Algorithm Audit's technical audit of a risk profiling-based control proces of a Dutch public sector organisation

{{< image id="technical-audit" width_desktop="4" width_mobile="12" image1="/images/algoprudence/AA202401/Cover.png" alt1="Technical audit" caption1="Technical audit" link1="/algoprudence/cases/aa202401_bias-prevented/" >}}

{{< button button_link="/algoprudence/cases/aa202401_bias-prevented/" button_text="Technical audit" >}}

{{< tab_content_close >}}

{{< tab_content_open icon="fa-newspaper" title="Administrative law" id="administrative-law" >}}

Unifying principles of sound administration with (semi-) automated decision-making is challenging. For instance:

Obligation to state reasons: Governmental institutions must always provide clear explanations for their decisions. However, when machine learning is employed, such as in variable selection for risk profiling, this transparency may be obscured. This leads to the question of how far arguments based on probability distributions are acceptable as explanations for why certain citizens are chosen for a particular profile.

[Read Algoprudence AA:2023:02 for a review of xgboost machine learning used for risk profiling variable selection ](http://localhost:1313/algoprudence/cases/risk-profiling-for-social-welfare-reexamination-aa202302/)

{{< tab_content_close >}}

{{< tab_content_open icon="fa-newspaper" title="FRIA" id="FRIA" >}}

The [Impact Assessment Human Rights and Algorithms (IAMA)](https://www.rijksoverheid.nl/documenten/rapporten/2021/02/25/impact-assessment-mensenrechten-en-algoritmes) and the [Handbook for Non-Discrimination](https://www.rijksoverheid.nl/documenten/rapporten/2021/06/10/handreiking-non-discriminatie-by-design), both developed by the Dutch government, assess discriminatory practice mainly by asking questions that are meant to stimulate self-reflection. It does not provide answers or concrete guidelines how to realise ethical algorithms.

{{< tab_content_close >}}

{{< tab_content_open icon="fa-newspaper" title="Registers" id="algorithm-registers" >}}

Unifying principles of sound administration with (semi-) automated decision-making is challenging. For instance:

Obligation to state reasons: Governmental institutions must always provide clear explanations for their decisions. However, when machine learning is employed, such as in variable selection for risk profiling, this transparency may be obscured. This leads to the question of how far arguments based on probability distributions are acceptable as explanations for why certain citizens are chosen for a particular profile.

[Read Algoprudence AA:2023:02 for a review of xgboost machine learning used for risk profiling variable selection ](http://localhost:1313/algoprudence/cases/risk-profiling-for-social-welfare-reexamination-aa202302/)

{{< tab_content_close >}}

{{< ai_policy_observatory >}}
