---
Banner:
  title: '_Algoprudence_:'
  image: images/main_illustration.svg
  content: Algorithm Audit is a European knowledge platform for AI bias testing and normative AI standards. We build and share _algoprudence_ by convening deliberative audit commissions. Our commissions give independent normative advice on ethical issues that arise in concrete use cases of algorithmic tools and methods
  button:
    enable: True
    label: Read our new white paper
    link: knowledge_base/white_paper_dsa_delegated_regulation_feedback/
What_we_do:
  enable: true
  title: Distinctive in
  feature_item:
  - name: Normative advice
    icon: fas fa-search
    content: Mindful of societal impact our audit commissions provide normative advice on ethical issues that arise in algorithmic use cases
  - name: Independence
    icon: fas fa-star-of-life
    content: By working nonprofit and under explicit terms and conditions, we ensure the independence, quality and diversity of our audit commissions
  - name: Ethics beyond compliance
    icon: fas fa-leaf
    content: We help organizations committed to ethical algorithms to make judgments about fairness and open legal norms
  - name: Public knowledge
    icon: fab fa-slideshare
    content: All our cases and corresponding advice (_algoprudence_) are made [<span style="color:#005aa7">publicly available</span>](/algoprudence), increasing collective knowledge how to deploy and use algorithms in an ethical way
  - name: Techno-ethical jurisprudence
    icon: fas fa-book-reader
    content: Stakeholders learn from our algoprudence, help to improve it and utilize it as a best practice in similar cases
  - name: Joint effort
    icon: fas fa-hands-helping
    content: Let’s remove boundaries between public and private organizations that face similar AI quandaries. We offer a collaborative platform for academics, activists, developers and policy makers to define normative standards for AI
Supported_by:
  enable: true
  Suported_by:
  - title: Supported by
    img_SIDN:
    - images/sidn.png
    img_EUAIS:
    - images/EUAISFund.png
    img_NLAIC:
    - images/NLAIC.png
    img_BZK:
    - images/BZK.jpg
    img_HAI:
    - images/HAI.png
How_we_work:
  enable: true
  service_item:
  - title: How we work
    images:
    - images/howwework.svg
    content: ''
    button:
      enable: true
      label: Algorithms we have reviewed
      link: algoprudence
With_whom_we_work:
  enable: true
  service_item:
  - title: Who we work with
    images:
    - images/howwework.svg
    content: We work together with international experts from various backgrounds,
      e.g. ethicists, legal professionals, data scientists. The composition of audit
      commissions varies per case. Most of the experts are affiliated with academic
      institutions. The Algorithm Audit team facilitates the procurement of sufficient
      background information about the case, after which the experts conduct an in-depth
      study, first individually and then collectively. Our team drafts a report that
      condenses the varied views of the commission.The report published by Algorithm
      Audit has been agreed upon by the commission members.
    button:
      enable: false
      label: Cases we work on
      link: "#/algoprudence"
Why_we_exist:
  enable: true
  content: Current legislation falls short to
  new_items:
  - title: Why we exist
    subtitle: EU AI policy observatory
    # menu
    tab1: AI Act
    tab2: DSA
    tab3: GDPR
    tab4: Administrative law
    tab5: HRIA
    tab6: AI registers
    tab7: Regulatory landscape 
    tab8: Adminis-trative law 
    tab9: AI registers

    # intro
    intro_content: AI ethics urgently needs case-based experience and a bottom-up approach. We believe existing and proposed legislation is and will not suffice to realize ethical algorithms. Why not? 
    
    #AI Act
    ai_act_title: AI Act
    ai_act_content:  The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1623335154975&uri=CELEX%3A52021PC0206" target="_blank">AI Act </a> imposes broad new responsibilities to control risks from AI systems without at the same time laying down specific standards they are expected to meet. For instance<span>:</span>
    ai_act_tag_conformity: Conformity assessment (Art. 43)<span>:</span>
    ai_act_content_conformity: The proposed route for internal control relies too much on the self-reflective capacities of producers to assess AI quality management, risk management and bias. Resulting in subjective best-practices;
    ai_act_tag_risk: Risk- and quality management systems (Art. 9 and 17)<span>:</span>
    ai_act_content_risk: Requirements set out for risk management systems and quality management systems remain too generic. For example, it does not provide precise guidelines how to identify and mitigate ethical issues such as algorithmic discrimination;
    ai_act_tag_standards: Normative standards<span>:</span>
    ai_act_content_standards: To realize AI harmonization across the EU, publicly available technical and normative best-practices for fair AI are urgently needed.
    ai_act_white_paper: Read our algoprudence on proxy discrimination
    ai_act_image: images/AIA.png

    #DSA
    dsa_title: Digital Service Act
    dsa_content: The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52020PC0825" target="_blank">Digital Services Act (DSA) </a> lacks provisions to disclose normative methodological choices that underlie general purpose AI systems. For instance<span>:</span>
    dsa_tag_risk: Risk definitions<span>:</span>
    dsa_content_risk: Article 9 of the <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits_en" target="_blank">Delegated Regulation (DR)</a> for independent third party auditing (as mandated under DSA Art. 37) specifies that “audit risk analysis shall consider _inherent risk_, _control risk_ and _detection risk_”. More specific guidance should be provided in Art. 2 of the DR how risks relating to subjective concepts, such as “...the nature, the activity and the use of the audited service”, can be assessed;  
    dsa_tag_template: Audit template<span>:</span>
    dsa_content_template: Pursuant to Article 5(1)(a) of the DR, Very Large Open Platforms (VLOPs) and Very Large Online Search Engines (VLOSEs) shall transmit to third-party auditing organisations “benchmarks used [...] to assert or monitor compliance [...], as well as supporting documentation”. We argue that the normative considerations underlying the selection of these benchmarks should be asked out more decisively in this phase of the audit. Therefore, we asked the European Commission (EC) to add this dimension to Question 3(a) of Section D.1 _Audit conclusion for obligation Subsection_ II. _Audit procedures and their results_;
    dsa_white_paper: Read our feedback to the Europen Commission on DSA Art. 37 Delegated Regulation  
    dsa_content_feedback: <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13626-Digital-Services-Act-conducting-independent-audits/feedback_en?p_id=32081201" target="_blank">Feedback</a> submitted to the European Commission (EC) on DSA Art. 37 DR showcasts that<span>:</span>
    dsa_content_feedback_tab1: Private auditors (like PwC and Deloitte) warn that the lack of guidance on criteria against which to audit poses a risk of subjective audits;
    dsa_content_feedback_tab2: Tech companies (like Snap and Wikipedia) raise concerns about the industry's lack of expertise to audit specific AI products, like company-tailored timeline recommender systems.
    dsa_image: images/DSA.jpg

    #GDPR
    gdpr_title: General Data Protection Regulation
    gdpr_content: The current data processing regulation only partially specifies measures to safeguard algorithmic decision-making. For instance<span>:</span> 
    gdpr_tag_dpia: Participatroy DPIA (art. 35 sub 9)<span>:</span>
    gdpr_content_dpia: This provision mandates that in cases where a Data Privacy Impact Assessment (DPIA) is obligatory, the opinions of data subjects regarding the planned data processing shall be seeked. This is a powerful legal mechanism to foster collaborative algorithm development. Nevertheless, the inclusion of data subjects in this manner is scarcely observed in practice;
    gdpr_tag_profiling: Profiling (recital 71)
    gdpr_content_profiling: is broadly defined as<span>:</span> “to analyse or predict aspects concerning the data subject’s performance at work, economic situation, health, personal preferences or interests, reliability or behaviour, location or movements”. However, the approval of profiling, particularly when “authorised by Union or Member State law to which the controller is subject, including fraud monitoring”, grants public and private entities significant flexibility to integrate algorithmic decision-making derived from diverse types of profiling. This wide latitude raises concerns about the potential for excessive consolidation of personal data and the consequences of algorithmic determinations;
    gdpr_tag_ADM: Automated decision-making (art. 22 sub 2)<span>:</span>
    gdpr_content_ADM: Allowing wide-ranging automated decision-making (ADM) and profiling under the sole condition of contract agreement opens the door for large scale unethical algorithmic practices without accountability and public awareness.
    gdpr_image: images/gdpr.svg

    #Administrative law
    administrative_law_title: Administrative law
    administrative_law_content: Unifying principles of sound administration with algorithmic methods is challenging. For instance<span>:</span>
    administrative_law_tag_motivation: Obligation to state reasons:</span>
    administrative_law_content_motivation: Decision made by governmental institutions should always be explainable. When using machine learning, for instance for variable selection for risk profiling, presents difficulties. To what extent are arguments based on probability distributions tolerable to be included in a motivation why citizens are selected by a certain profile?
    algoprudence_rotterdam: Read Algoprudence AA:2023:02 for a review of xgboost machine learning used for risk profiling variable selection
    administrative_law_image: images/Awb.jpg

    #Regulatory landscape
    regulator_title: Regulatory landscape
    regulator_content: <a href="https://www.rekenkamer.nl/onderwerpen/algoritmes-digitaal-toetsingskader/ethiek" target="_blank">Perspective 3.1.1</a> in the Guidelines for Algorithms of the Dutch Court of Auditors argues that
      ethical algorithms are not allowed to “discriminate and that bias should be
      minimised”. Missing from this judgment is a discussion of what precisely constitutes
      bias in the context of algorithms and what would be appropriate methods to ascertain
      and mitigate algorithmic discrimination. In the absence of a clear ethical framework, it
      is up to organizations to formulate context-sensitive approaches to combat discrimination.
    regulator_image: images/FRIA.png

    #Human rights and fundamental rights impact assessments
    hria_title: Fundamental Rights and Human Rights Impact Assessments
    hria_content: The <a href="https://www.rijksoverheid.nl/documenten/rapporten/2021/02/25/impact-assessment-mensenrechten-en-algoritmes" target="_blank">Impact Assessment Human Rights and Algorithms (IAMA)</a> and the <a href="https://www.rijksoverheid.nl/documenten/rapporten/2021/06/10/handreiking-non-discriminatie-by-design" target="_blank">Handbook for
      Non-Discrimination</a>, both developed by the Dutch government, assess discriminatory
      practice mainly by asking questions that are meant to stimulate self-reflection.
      It does not provide answers or concrete guidelines how to realise ethical algorithms.
    hria_image: images/FRIA.png

    #AI registers
    AI_register_title: AI registers
    AI_register_content: ..
    AI_register_image: images/knowledge_base.svg

    #Archive
    content1:  We believe a case-based and context sensitive approach is indispensable to develop ethical algorithms. One should not expect top-down regulation and
      legislation to solve all ethical problems in AI and machine learning. Taking
      all contested algorithmic cases to court is practically infeasible. More importantly,
      organizations will always carry their own responsibility for ethical algorithms
      within and beyond the obligation of legal compliance. Hence, new bottom-up initiatives
      like Algorithm Audit are necessary to support these organizations and to strengthen
      ethical practice in ADM and decision support.
      

      We provide a platform where experts
      in AI ethics from various disciplines can interact with, learn from and steer actual algorithmic practice
      and surrounding ethical concerns. We increase public knowledge and stimulate
      an informed and open debate about what ethical algorithms we desire as a society in various
      contexts. Our audit commissions shape future public values through discussion and deliberation. As such, Algorithm Audit contributes in the digital realm to SDG16 – Peace, Justice and
      Strong Institutions. 
Get_in_touch:
  enable: true
  title: Get in touch
  content: Do you have an ethical issue for review? Or want to share ideas? Let us
    know!

---
